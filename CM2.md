# CM2 - Partie 1 : Gen√®se et architecture interne de Kubernetes

---

## 1. Origine et philosophie de Kubernetes

Kubernetes est n√© au sein de **Google** en 2014, inspir√© d'un outil interne appel√© **Borg**. Borg √©tait l'orchestrateur interne qui g√©rait des millions de conteneurs au sein de l'infrastructure Google. Kubernetes en reprend les id√©es fondamentales tout en les ouvrant √† la communaut√© open source.

L'objectif de Kubernetes est de **g√©rer automatiquement le cycle de vie d'applications conteneuris√©es**, quel que soit l'environnement (local, cloud, hybride).

> Kubernetes n'ex√©cute pas vos applications directement ‚Äî il orchestre leur ex√©cution sur un ensemble de machines.

### Principes fondateurs

1. **D√©claratif** : on d√©crit _l'√©tat souhait√©_, Kubernetes s'assure que le syst√®me y converge.
2. **Automatis√©** : planification, r√©cup√©ration et red√©ploiement sans intervention humaine.
3. **Distribu√©** : compos√© de plusieurs services coop√©rants par API.
4. **Extensible** : chaque composant peut √™tre remplac√© ou augment√© par un contr√¥leur personnalis√©.

---

## 2. Le cluster Kubernetes

Un cluster Kubernetes est constitu√© de deux grandes parties :

### üß† Le **Control Plane** (Plan de contr√¥le)

C'est le cerveau du cluster. Il prend les d√©cisions d'orchestration et maintient la coh√©rence de l'√©tat global.

| Composant                        | R√¥le                                                                                       |
| -------------------------------- | ------------------------------------------------------------------------------------------ |
| **API Server**                   | Point d'entr√©e unique du cluster. Re√ßoit toutes les requ√™tes `kubectl` ou des contr√¥leurs. |
| **etcd**                         | Base de donn√©es cl√©/valeur distribu√©e, stocke l'√©tat souhait√© du cluster.                  |
| **Controller Manager**           | Compare l'√©tat r√©el et l'√©tat d√©sir√©, et agit en cons√©quence.                              |
| **Scheduler**                    | D√©termine sur quel n≈ìud ex√©cuter chaque Pod selon les ressources disponibles.              |
| **Cloud Controller (optionnel)** | Int√®gre Kubernetes avec les APIs d'un fournisseur cloud.                                   |

### ‚öôÔ∏è Les **Worker Nodes** (N≈ìuds de travail)

Chaque n≈ìud ex√©cute les conteneurs r√©els. Il contient :

| Composant             | R√¥le                                                                                   |
| --------------------- | -------------------------------------------------------------------------------------- |
| **kubelet**           | Agent local. Re√ßoit les instructions du Control Plane et g√®re les Pods sur la machine. |
| **container runtime** | Ex√©cute les conteneurs (Docker, containerd, CRI-O...).                                 |
| **kube-proxy**        | G√®re le routage et les r√®gles de communication r√©seau entre Pods et Services.          |

---

## 3. Communication interne et API

Tout dans Kubernetes passe par **l'API Server**. Il s'agit d'un serveur HTTP/JSON qui re√ßoit des requ√™tes REST :

- `kubectl get pods` ‚Üí **GET /api/v1/pods**
- `kubectl apply -f deploy.yml` ‚Üí **POST /apis/apps/v1/deployments**

Les autres composants (Controller, Scheduler, Kubelet) **ne communiquent qu'avec l'API Server**, jamais directement entre eux. Ce mod√®le permet :

- une **isolation fonctionnelle** entre les processus,
- un **contr√¥le d'acc√®s centralis√©**,
- une **extensibilit√©** naturelle (Custom Controllers, CRDs).

> Kubernetes expose une API unique, que l‚Äôon interroge et modifie comme une base de donn√©es d‚Äô√©tat du cluster.

---

## 4. Le cycle de r√©conciliation

Kubernetes fonctionne selon une logique de **r√©conciliation continue** :

```mermaid
flowchart LR
    A["√âtat d√©sir√© (YAML)"] -->|kubectl apply| B["API Server"]
    B --> C["etcd (base cl√©-valeur)"]
    C --> D["Controller Manager"]
    D --> E{"√âtat r√©el == √âtat d√©sir√© ?"}
    E -->|Non| F["Scheduler + Kubelet cr√©ent ou modifient des Pods"]
    E -->|Oui| G["Cluster stable"]
```

### Les √©tapes principales

1. L'utilisateur applique un **manifest YAML** (ex : Deployment).
2. L'**API Server** valide et enregistre la ressource dans `etcd`.
3. Le **Controller Manager** d√©tecte qu'aucun Pod n'existe encore pour ce Deployment.
4. Le **Scheduler** assigne les Pods √† des n≈ìuds.
5. Les **kubelets** cr√©ent les conteneurs correspondants.
6. Le cluster atteint un **√©tat stable** lorsque la r√©alit√© correspond au YAML.

> Cette boucle tourne en permanence. Kubernetes s'auto-r√©pare d√®s qu'un √©cart est d√©tect√© (self-healing).

---

## 5. Sch√©ma d'ensemble

```mermaid
graph TB
  subgraph ControlPlane[Control Plane]
    APIServer[(API Server)] --> ETCD[(etcd)]
    APIServer --> Controller[Controller Manager]
    APIServer --> Scheduler[Scheduler]
  end

  subgraph Node1[Worker Node 1]
    Kubelet1[kubelet] --> Pod1[Pod A]
    KubeProxy1[kube-proxy]
  end

  subgraph Node2[Worker Node 2]
    Kubelet2[kubelet] --> Pod2[Pod B]
    KubeProxy2[kube-proxy]
  end

  Controller -->|Ordres de cr√©ation| Kubelet1
  Controller -->|Ordres de cr√©ation| Kubelet2
  Kubelet1 -->|Statut| APIServer
  Kubelet2 -->|Statut| APIServer

  KubeProxy1 -->|Trafic r√©seau| KubeProxy2
```

---

## 6. Points √† retenir

- Kubernetes est un **syst√®me distribu√©** bas√© sur une API unique.
- Le **Control Plane** d√©cide, les **n≈ìuds appliquent**.
- L'√©tat du cluster est stock√© dans **etcd**, source de v√©rit√©.
- Le **Scheduler** et le **Controller Manager** maintiennent la coh√©rence.
- Le **kubelet** est le lien entre la th√©orie (manifest YAML) et la r√©alit√© (conteneurs en ex√©cution).

---

**Prochaine partie :** Le mod√®le d√©claratif et la structure des manifests YAML.

# CM2 - Partie 2 : Le mod√®le d√©claratif et les manifests YAML

---

## 1. Le mod√®le d√©claratif de Kubernetes

Kubernetes repose sur une approche **d√©clarative**. Cela signifie que l‚Äôadministrateur ou le d√©veloppeur **ne d√©crit pas les √©tapes √† ex√©cuter**, mais **l‚Äô√©tat final souhait√©** du syst√®me.

Autrement dit :

> On d√©clare _ce que l‚Äôon veut obtenir_, pas _comment l‚Äôobtenir_.

C‚Äôest le **Control Plane** (API Server + Controller Manager + Scheduler) qui se charge de faire converger le cluster vers cet √©tat.

### ‚öôÔ∏è Exemple conceptuel

Imaginons que vous √©criviez :

```yaml
replicas: 3
```

Cela ne dit pas _comment_ cr√©er trois conteneurs, mais indique simplement √† Kubernetes :

> ¬´ Assure-toi qu‚Äôil y ait toujours trois r√©plicas de ce Pod actifs. ¬ª

Si un conteneur tombe, Kubernetes en relance un automatiquement pour maintenir cet √©tat.

### üß† Ce que cela implique

- L‚Äôutilisateur **n‚Äôex√©cute pas des commandes d‚Äôinfrastructure** ; il **modifie des objets dans une base d‚Äô√©tat**.
- L‚Äôorchestrateur **surveille en continu** les √©carts entre l‚Äô√©tat d√©sir√© et l‚Äô√©tat r√©el.
- Le cluster est donc **auto-r√©parant** (self-healing) et **auto-adaptatif** (scaling, update, rollback).

---

## 2. Le r√¥le du YAML dans cette logique

Le YAML (Yet Another Markup Language) est le format privil√©gi√© pour d√©crire les ressources Kubernetes. Il agit comme **la syntaxe du contrat** entre le d√©veloppeur et le cluster.

Chaque fichier YAML repr√©sente un ou plusieurs **objets Kubernetes** (souvent appel√©s _manifests_).

### üß© Structure de base

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  MODE: "production"
  TIMEOUT: "30"
```

Cette ressource exprime l‚Äôintention : ¬´ cr√©e un objet de type ConfigMap nomm√© `app-config` contenant des donn√©es de configuration. ¬ª

### Les quatre sections fondamentales

| Cl√©          | R√¥le                                  | Exemple                                  |
| ------------ | ------------------------------------- | ---------------------------------------- |
| `apiVersion` | indique la version de l‚ÄôAPI utilis√©e  | `v1`, `apps/v1`, `batch/v1`              |
| `kind`       | type d‚Äôobjet √† cr√©er                  | `Pod`, `Service`, `Deployment`, `Job`... |
| `metadata`   | identifiants (nom, labels, namespace) | `name: webapp`, `labels: app=web`        |
| `spec`       | sp√©cification fonctionnelle (le c≈ìur) | `replicas`, `containers`, `ports`, etc.  |

### üß± Notion de hi√©rarchie et indentation

L‚Äôindentation en YAML refl√®te la structure de l‚Äôobjet : chaque niveau d√©crit un sous-champ ou un bloc d‚Äôattributs. **Les espaces sont obligatoires** et **les tabulations interdites**.

```yaml
spec:
  replicas: 2
  selector:
    matchLabels:
      app: web
```

> En YAML, l‚Äôespace **porte le sens**. Une simple erreur d‚Äôindentation change la signification du document.

---

## 3. Du manifest au cluster : la cha√Æne de traitement

```mermaid
flowchart LR
    A["Manifest YAML"] --> B["API Server"]
    B --> C["etcd (base cl√©-valeur)"]
    C --> D["Controller Manager"]
    D --> E["Scheduler"]
    E --> F["Kubelet (sur le n≈ìud)"]
    F --> G["Conteneur cr√©√©"]
```

### üîç √âtapes d√©taill√©es

1. **kubectl apply -f fichier.yaml** ‚Üí Envoie le manifest √† l‚ÄôAPI Server.
2. **Validation** ‚Üí L‚ÄôAPI v√©rifie la conformit√© (syntaxe, permissions, version d‚ÄôAPI).
3. **Stockage dans etcd** ‚Üí L‚Äô√©tat souhait√© est √©crit dans la base d‚Äô√©tat.
4. **Controller Manager** ‚Üí Compare cet √©tat √† la r√©alit√© et cr√©e les objets manquants.
5. **Scheduler** ‚Üí Assigne chaque Pod √† un n≈ìud selon les ressources.
6. **kubelet** ‚Üí T√©l√©charge l‚Äôimage, cr√©e le conteneur, et signale le statut.

Le syst√®me entre alors dans sa boucle de **r√©conciliation continue**.

> Kubernetes se comporte comme un automate d‚Äô√©tat : tant que la r√©alit√© diff√®re du manifest, il agit.

---

## 4. `kubectl create` vs `kubectl apply`

Il est fr√©quent de confondre les commandes `create` et `apply`. Elles semblent similaires, mais elles participent toutes deux du mod√®le d√©claratif.

### ‚ú≥Ô∏è `kubectl create`

Cr√©e un nouvel objet √† partir d‚Äôun fichier ou d‚Äôun mod√®le direct¬†:

```bash
kubectl create deployment web --image=nginx
```

Cette commande **g√©n√®re implicitement un manifest YAML** qui est imm√©diatement transmis √† l‚ÄôAPI Server. L‚Äôobjet cr√©√© devient alors une entr√©e dans `etcd`, et le Controller Manager se charge de sa r√©conciliation comme pour tout autre objet.

On peut ensuite **retrouver la description d√©clarative** de cet objet :

```bash
kubectl get deployment web -o yaml
```

> M√™me si la commande `create` semble imp√©rative, Kubernetes la traduit en une **d√©claration d‚Äô√©tat**. Ce qui est sauvegard√© et r√©concili√©, c‚Äôest un objet YAML, pas un ordre ex√©cutable.

### üß© `kubectl apply`

Applique ou met √† jour un manifest existant. Il cr√©e l‚Äôobjet s‚Äôil n‚Äôexiste pas encore, sinon il modifie uniquement les champs n√©cessaires pour atteindre l‚Äô√©tat souhait√©.

```bash
kubectl apply -f deployment.yaml
```

Kubernetes conserve une copie de la configuration appliqu√©e (dans l‚Äôannotation `kubectl.kubernetes.io/last-applied-configuration`), ce qui permet de **comparer les diff√©rences** et d‚Äô**effectuer des mises √† jour progressives**.

### üí° En r√©sum√©

| Commande | Nature                  | Utilisation typique                  | Gestion de l‚Äô√©tat                 |
| -------- | ----------------------- | ------------------------------------ | --------------------------------- |
| `create` | D√©claratif (instantan√©) | Cr√©er une ressource unique ou rapide | Enregistrement imm√©diat dans etcd |
| `apply`  | D√©claratif (continu)    | Cr√©er ou mettre √† jour via YAML      | Suivi et r√©conciliation continue  |

Dans les deux cas, Kubernetes fonctionne toujours selon le m√™me principe¬†: le **Controller Manager** compare en permanence l‚Äô√©tat d√©clar√© √† l‚Äô√©tat observ√© et agit jusqu‚Äô√† leur convergence.

---

## 5. Typologie des objets Kubernetes

Tous les objets Kubernetes d√©rivent de la m√™me structure, mais ils ont des finalit√©s diff√©rentes.

| Cat√©gorie            | Type d‚Äôobjet                                  | R√¥le                                           |
| -------------------- | --------------------------------------------- | ---------------------------------------------- |
| **Workload**         | Pod, ReplicaSet, Deployment, StatefulSet, Job | D√©ploient et maintiennent des conteneurs       |
| **Service & R√©seau** | Service, Ingress, Endpoint                    | Assurent la connectivit√© interne/externe       |
| **Configuration**    | ConfigMap, Secret, Volume                     | Fournissent des donn√©es de configuration       |
| **Infrastructure**   | Node, Namespace, PV, PVC                      | D√©crivent les ressources physiques ou logiques |
| **S√©curit√©**         | NetworkPolicy, ServiceAccount, Role, RBAC     | G√®rent l‚Äôacc√®s et l‚Äôisolation                  |

### Exemple : un Deployment complet

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-deploy
  labels:
    app: web
spec:
  replicas: 2
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      containers:
        - name: web
          image: nginx:1.25
          ports:
            - containerPort: 80
```

> Ce manifest d√©clare l‚Äôexistence d‚Äôun ensemble de Pods g√©r√©s par un contr√¥leur de type Deployment. Le champ `replicas` indique la quantit√© attendue, et `selector` lie ce Deployment aux Pods correspondants.

---

## 6. Manifests multi-documents

Il est possible de combiner plusieurs ressources dans un seul fichier s√©par√© par `---`.

```yaml
---
apiVersion: v1
kind: Namespace
metadata:
  name: demo
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web
  namespace: demo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      containers:
        - name: web
          image: nginx:1.25
---
apiVersion: v1
kind: Service
metadata:
  name: web-svc
  namespace: demo
spec:
  selector:
    app: web
  ports:
    - port: 80
      targetPort: 80
```

> Cette approche permet de d√©ployer un mini-√©cosyst√®me complet en une seule commande.

```bash
kubectl apply -f stack-demo.yaml
```

---

## 7. V√©rification et inspection

Quelques commandes essentielles pour observer l‚Äô√©tat du cluster apr√®s d√©ploiement :

| Action                | Commande                                  | R√©sultat                          |
| --------------------- | ----------------------------------------- | --------------------------------- |
| Lister les ressources | `kubectl get all -n demo`                 | Tous les objets du namespace      |
| D√©tails d‚Äôun objet    | `kubectl describe deployment web -n demo` | Informations, √©v√©nements          |
| Contenu YAML actuel   | `kubectl get deploy web -o yaml`          | √âtat courant du manifest appliqu√© |
| √âtat des Pods         | `kubectl get pods -o wide`                | IPs, n≈ìuds, statuts               |

Ces commandes permettent de relier la **th√©orie (fichier YAML)** √† la **r√©alit√© (conteneurs actifs)**.

---

## 8. Points p√©dagogiques cl√©s

- Kubernetes est **pilot√© par des manifests** : tout est un objet API d√©crit en YAML.
- L‚Äôutilisateur **ne manipule pas les conteneurs**, mais les objets qui les repr√©sentent.
- Le **Control Plane** maintient la coh√©rence entre l‚Äô√©tat d√©sir√© et l‚Äô√©tat r√©el.
- Le YAML est **la grammaire du dialogue** entre l‚Äôhumain et l‚Äôorchestrateur.
- M√™me les commandes `create` sont **d√©claratives**, car elles produisent un objet persistant que le Controller Manager r√©concilie.

---

**Prochaine partie :** Isolation logique, exposition r√©seau et structure applicative (Namespaces, Services, Ingress).

# CM2 ‚Äì Partie 3 : Isolation, organisation et exposition dans Kubernetes

---

## 1. Introduction : du manifest √† la structure du cluster

Apr√®s avoir d√©couvert comment d√©crire les objets Kubernetes (Pods, Deployments, Services) via YAML, nous abordons maintenant la **structure logique et r√©seau** du cluster.

Kubernetes est un syst√®me **d√©claratif et auto-r√©gul√©** : le Control Plane s'assure que l'√©tat du cluster correspond en permanence √† l'√©tat d√©clar√©. Cette r√©conciliation s'applique non seulement aux objets applicatifs, mais √©galement √† la fa√ßon dont les applications sont organis√©es, expos√©es et isol√©es.

Dans cette partie, nous allons comprendre comment Kubernetes :

- **organise** les ressources avec les namespaces ;
- **connecte** les composants via les services internes ;
- **expose** les applications vers l‚Äôext√©rieur avec les ingress ;
- **prot√®ge** le trafic avec les politiques r√©seau.

---

## 2. Les Namespaces : l'isolation logique du cluster

### 2.1. R√¥le fondamental

Un **namespace** est un espace logique d'isolation √† l'int√©rieur d'un cluster. Il ne sert **pas √† repr√©senter un environnement (dev/test/prod)**, mais √† **segmenter et organiser** les ressources en fonction de domaines fonctionnels, d'√©quipes, ou d'applications.

Chaque namespace constitue un **domaine d'administration et de s√©curit√©** distinct :

- les noms de ressources doivent y √™tre uniques ;
- les r√®gles d'acc√®s (RBAC) et quotas peuvent √™tre appliqu√©s de mani√®re sp√©cifique ;
- certaines ressources (Nodes, ClusterRoles, PersistentVolumes) sont globales et ne d√©pendent d'aucun namespace.

> ‚ú® Un cluster Kubernetes = une ville partag√©e. Les namespaces = les quartiers isol√©s avec leurs propres r√®gles, habitants et ressources.

### 2.2. Bonnes pratiques d'organisation

Plut√¥t que d'utiliser les namespaces pour simuler des environnements, on les emploie pour :

- **isoler des applications** : `frontend`, `backend`, `database`, `monitoring` ;
- **isoler des √©quipes ou clients** : `team-a`, `client-x`, `client-y` ;
- **g√©rer la s√©curit√© et la gouvernance** : attribution de r√¥les, quotas, limites CPU/m√©moire, secrets.

En revanche, les environnements **dev / staging / prod** devraient √™tre **sur des clusters distincts** pour √©viter les fuites de s√©curit√© et garantir la r√©silience.

### 2.3. Commandes pratiques

```bash
kubectl get ns
kubectl create ns analytics
kubectl config set-context --current --namespace=analytics
kubectl get all -n analytics
```

### 2.4. Namespace par d√©faut et objets globaux

Si aucun namespace n'est sp√©cifi√©, Kubernetes cr√©e l'objet dans le namespace **default**.
Certains objets sont dits **non namespac√©s** (globaux) :

- `Node`, `Namespace`, `ClusterRole`, `PersistentVolume`.

---

### 2.5. Discussion critique : Namespaces, Zero Trust et architecture

Une id√©e fausse tr√®s r√©pandue consiste √† assimiler les namespaces √† des environnements (dev/test/prod). En r√©alit√© :

- Kubernetes est **une plateforme de production**, pas un outil de sandbox pour le d√©veloppement.
- Les environnements s√©par√©s reposent sur des **clusters distincts**, pas des namespaces.

#### üîí R√¥le s√©curitaire des namespaces

- Les namespaces servent √† **limiter la surface d'attaque** en cloisonnant les ressources et les acc√®s.
- Les communications inter-namespaces peuvent √™tre **restreintes** par des `NetworkPolicies`.
- Dans une approche **Zero Trust**, rien n'est implicitement autoris√©.

#### üïµÔ∏è‚Äç‚ôÇÔ∏è Sch√©mas d'isolation possibles

1. **Namespace unique** : tous les composants d'une application dans un seul espace (simple, mais expos√©).
2. **Multi-namespaces applicatifs** : front / back / data dans des namespaces diff√©rents avec des r√®gles de communication pr√©cises.
3. **Multi-clusters (production)** : un cluster par environnement (dev/staging/prod) pour une isolation totale.

> En production, c'est la troisi√®me approche qui correspond √† la r√©alit√© cloud-native.

---

## 3. Les Services : connecter et stabiliser le r√©seau interne

### 3.1. Probl√©matique

Les Pods sont √©ph√©m√®res : leurs adresses IP changent √† chaque recr√©ation. Les Services assurent une **adresse stable et un m√©canisme de d√©couverte interne (DNS)**.

### 3.2. Types de Services

| Type           | Port√©e            | R√¥le                                   | Cas d‚Äôusage                       |
| -------------- | ----------------- | -------------------------------------- | --------------------------------- |
| `ClusterIP`    | Interne           | Point d'acc√®s interne au cluster       | Communication entre microservices |
| `NodePort`     | Interne + externe | Exposition sur chaque n≈ìud (port fixe) | D√©monstrations locales            |
| `LoadBalancer` | Externe (cloud)   | Exposition publique                    | Production                        |
| `ExternalName` | DNS externe       | Redirection vers ressource externe     | API, base de donn√©es distante     |

### 3.3. Exemple YAML

```yaml
apiVersion: v1
kind: Service
metadata:
  name: api-svc
  namespace: backend
spec:
  selector:
    app: api
  ports:
    - port: 8080
      targetPort: 8080
  type: ClusterIP
```

### 3.4. R√©solution DNS interne

Chaque Service obtient automatiquement un nom DNS :
`api-svc.backend.svc.cluster.local`

Les Pods peuvent ainsi se contacter via le nom du Service, sans jamais conna√Ætre son adresse IP.

> Le DNS interne est g√©r√© par **CoreDNS**, d√©ploy√© comme un Pod dans le namespace `kube-system`.

---

## 4. L‚ÄôIngress : exposer les applications vers l‚Äôext√©rieur

### 4.1. Pourquoi un Ingress ?

Les Services expos√©s en `NodePort` ou `LoadBalancer` sont pratiques mais limit√©s :

- `NodePort` n√©cessite un port ouvert sur chaque n≈ìud,
- `LoadBalancer` d√©pend du cloud provider.

‚ú® L'objet `Ingress` d√©crit des **r√®gles de routage HTTP(S)** g√©r√©es par un **Ingress Controller** (Traefik, Nginx, Istio Gateway, etc.).

### 4.2. Exemple YAML

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: web-ingress
  namespace: frontend
  annotations:
    kubernetes.io/ingress.class: traefik
spec:
  rules:
    - host: web.example.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: web-svc
                port:
                  number: 80
```

### 4.3. Sch√©ma logique

```mermaid
graph LR
User["Navigateur web"] --> Ingress["Ingress Controller (Traefik/Nginx)"]
Ingress --> Service["Service ClusterIP"]
Service --> Pod1["Pod web-1"]
Service --> Pod2["Pod web-2"]
```

> L'Ingress agit comme un proxy inverse HTTP/TLS, appliquant les r√®gles de routage et les certificats.

---

## 5. L‚ÄôEgress et les NetworkPolicies : contr√¥ler le trafic sortant

### 5.1. Par d√©faut : tout est ouvert

Dans Kubernetes, les Pods peuvent, par d√©faut, communiquer librement vers l'ext√©rieur et entre eux.

Pour des raisons de s√©curit√© et de conformit√©, il est essentiel de **fermer ces canaux par d√©faut** et d'appliquer le principe du moindre privil√®ge.

### 5.2. Exemple de NetworkPolicy

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: restrict-egress
  namespace: backend
spec:
  podSelector:
    matchLabels:
      app: api
  policyTypes:
    - Egress
  egress:
    - to:
        - ipBlock:
            cidr: 10.0.0.0/8
```

> Cette politique interdit tout trafic sortant sauf vers le r√©seau interne 10.0.0.0/8.

### 5.3. Principes √† retenir

- `Ingress` = flux **entrant** vers les Pods.
- `Egress` = flux **sortant** des Pods.
- `NetworkPolicy` = pare-feu logique au niveau du Pod.
- En Zero Trust, tout est **ferm√© par d√©faut**, puis **ouvert par exception**.

---

## 6. Sch√©ma r√©capitulatif

```mermaid
graph TD
  subgraph Cluster["Cluster Kubernetes"]
    subgraph NamespaceA["Namespace: frontend"]
      I["Ingress Controller"]
      S1["Service web-svc"]
      P1["Pod web-1"]
      P2["Pod web-2"]
    end
    subgraph NamespaceB["Namespace: backend"]
      S2["Service api-svc"]
      P3["Pod api-1"]
      P4["Pod api-2"]
    end
  end
  User["Navigateur"] --> I --> S1 --> P1 & P2
  P1 --> S2 --> P3
  P3 -.->|"Egress contr√¥l√©"| ExternalDB["Base de donn√©es externe (SaaS)"]
```

---

## 7. Conclusion de la Partie 3

- Les **namespaces** d√©limitent des domaines d'administration et de s√©curit√© au sein d'un cluster.
- Les **services** stabilisent les connexions internes entre pods.
- Les **ingress** exposent les applications vers l'ext√©rieur via un contr√¥leur HTTP/TLS.
- Les **network policies** assurent un contr√¥le fin des flux entrants et sortants.

Ensemble, ces m√©canismes composent une **infrastructure segment√©e, d√©clarative et s√©curis√©e**, orchestr√©e par le **Control Plane** de Kubernetes.

> Le TD4 prolongera cette approche avec la mise en place concr√®te de **Traefik (Ingress Controller)**, l'exploration du **r√©seau inter-pods**, et l'observation en temps r√©el via **Lens** et **Prometheus**.
