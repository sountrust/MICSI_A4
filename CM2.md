# CM2 ‚Äì Structure, isolation et exposition dans Kubernetes

## üìò Du YAML au r√©seau : comprendre la logique d√©clarative de K8s

---

## 1. Pourquoi YAML ?

### Le langage de la d√©claration d'√©tat

Kubernetes est **d√©claratif** : on ne d√©crit pas _comment faire_, mais _ce que l‚Äôon veut obtenir_.
Le fichier YAML (Yet Another Markup Language) permet d‚Äôexprimer cet **√©tat d√©sir√©**.

### Principe de base

- Chaque fichier YAML d√©crit **un ou plusieurs objets Kubernetes**.
- Chaque objet poss√®de trois cl√©s fondamentales :
  - `apiVersion` ‚Üí version de l‚ÄôAPI utilis√©e
  - `kind` ‚Üí type d‚Äôobjet (Pod, Deployment, Service, etc.)
  - `metadata` ‚Üí informations descriptives (nom, labels, namespace)
  - `spec` ‚Üí le c≈ìur fonctionnel (comportement de l‚Äôobjet)

### Exemple minimal : ConfigMap

```yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: demo-config
data:
  message: "Bonjour Kubernetes"
  version: "1.0"
```

> Ce fichier d√©clare une ressource de type ConfigMap appel√©e `demo-config` qui contient deux cl√©s/valeurs.

---

## 2. R√®gles de base YAML

### Couples cl√©/valeur

Chaque cl√© est suivie de `:` et d‚Äôun espace.

```yaml
apiVersion: "v1"
data: "42"
```

Les guillemets √©vitent les erreurs de typage (ex. `version: "1.0"` au lieu de `1.0`).

### Tableaux et dictionnaires

**Liste :**

```yaml
ports:
  - 80
  - 443
```

**Objet imbriqu√© :**

```yaml
metadata:
  name: web
  labels:
    app: demo
```

Les **espaces sont obligatoires** (pas de tabulations).
L‚Äôindentation = hi√©rarchie logique.

### YAML multi-documents

Un fichier YAML peut contenir **plusieurs objets** s√©par√©s par `---`.

```yaml
---
kind: Namespace
metadata:
  name: demo
---
kind: Deployment
metadata:
  name: web
  namespace: demo
```

Cela permet d‚Äôappliquer plusieurs ressources en une seule commande :

```bash
kubectl apply -f demo-stack.yml
```

---

## 3. Pourquoi plusieurs fichiers dans un d√©ploiement ?

Kubernetes d√©ploie des syst√®mes **modulaires et interconnect√©s** :
chaque objet a une **responsabilit√© distincte**.

| Objet         | R√¥le                                   | Exemple         |
| ------------- | -------------------------------------- | --------------- |
| Namespace     | Isolation logique                      | `namespace.yml` |
| Deployment    | D√©crit les Pods (containers, replicas) | `app.yml`       |
| Service       | Acc√®s r√©seau interne                   | `service.yml`   |
| Ingress       | Acc√®s externe HTTP                     | `ingress.yml`   |
| NetworkPolicy | S√©curit√© r√©seau                        | `policy.yml`    |

### Logique s√©quentielle du d√©ploiement

```mermaid
flowchart TD
  A["Namespace (isolation)"] --> B["Deployment (Pods)"]
  B --> C["Service (exposition interne)"]
  C --> D["Ingress (routage HTTP externe)"]
  D --> E["Egress (sortie r√©seau contr√¥l√©e)"]
```

> Chaque √©tape ajoute une "couche de visibilit√©" au sein du cluster.

---

## 4. L‚Äôisolation avec les Namespaces

Les **namespaces** permettent de segmenter le cluster en environnements logiques :

- Par √©quipe (`dev`, `test`, `prod`)
- Par projet (`frontend`, `backend`)
- Par niveau de s√©curit√©

### Commandes pratiques

```bash
kubectl get ns
kubectl create ns demo
kubectl config set-context --current --namespace=demo
```

**Observation :**

```bash
kubectl get all -n demo
kubectl get pods -A    # tous les namespaces
```

> Toujours sp√©cifier le namespace dans vos fichiers :

```yaml
metadata:
  name: web
  namespace: demo
```

---

## 5. Exemple de d√©ploiement complet (multi-fichiers)

### namespace.yml

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: demo
```

### deployment.yml

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web
  namespace: demo
spec:
  replicas: 2
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      containers:
        - name: web
          image: nginx:1.25
          ports:
            - containerPort: 80
```

### service.yml

```yaml
apiVersion: v1
kind: Service
metadata:
  name: web-svc
  namespace: demo
spec:
  selector:
    app: web
  ports:
    - port: 80
      targetPort: 80
  type: ClusterIP
```

### ingress.yml

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: web-ing
  namespace: demo
  annotations:
    kubernetes.io/ingress.class: traefik
spec:
  rules:
    - host: web.local
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: web-svc
                port:
                  number: 80
```

### networkpolicy.yml

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: restrict-egress
  namespace: demo
spec:
  podSelector:
    matchLabels:
      app: web
  policyTypes:
    - Egress
  egress:
    - to:
        - ipBlock:
            cidr: 192.168.0.0/16
```

---

## 6. V√©rification √©tape par √©tape

| √âtape                 | Commande                          | Attendu                     |
| --------------------- | --------------------------------- | --------------------------- |
| Cr√©ation du namespace | `kubectl apply -f namespace.yml`  | `namespace/demo created`    |
| D√©ploiement app       | `kubectl apply -f deployment.yml` | Pods cr√©√©s                  |
| V√©rification          | `kubectl get pods -n demo`        | READY = 2/2                 |
| Ajout du service      | `kubectl apply -f service.yml`    | Service `web-svc` actif     |
| Test r√©seau interne   | `kubectl get svc -n demo`         | IP interne ClusterIP        |
| Ajout de l‚Äôingress    | `kubectl apply -f ingress.yml`    | Routage web disponible      |
| Observation finale    | `kubectl get all -n demo`         | Pods, Svc, Ingress visibles |

---

## 7. Sch√©ma r√©capitulatif de la pile r√©seau Kubernetes

```mermaid
graph LR
  subgraph Cluster "Cluster Kubernetes (namespace: demo)"
    subgraph Node1
      P1["Pod: web-1"]
      P2["Pod: web-2"]
    end
    subgraph Service["Service ClusterIP (web-svc)"]
      S1["Load-balancing interne"]
    end
    subgraph Ingress["Ingress Controller (Traefik)"]
      I1["R√®gles HTTP / DNS"]
    end
  end
  User["Navigateur : http://web.local"] --> I1 --> S1 --> P1 & P2
```

---

## 8. Pour aller plus loin : Egress et Policies

Les **NetworkPolicies** permettent :

- De limiter le trafic sortant (Egress)
- De restreindre les communications entrantes (Ingress)
- De renforcer la s√©curit√© inter-pods

En production, ces r√®gles sont essentielles pour :

- isoler les namespaces,
- emp√™cher la fuite de donn√©es,
- restreindre les acc√®s √† certaines IP internes.

---

## 9. Synth√®se

| Couche | Objet         | R√¥le                             |
| ------ | ------------- | -------------------------------- |
| 1      | Namespace     | Isolation logique                |
| 2      | Deployment    | Gestion des Pods                 |
| 3      | Service       | Communication interne            |
| 4      | Ingress       | Exposition externe               |
| 5      | NetworkPolicy | S√©curit√© r√©seau (egress/ingress) |

---

## 10. Conclusion du CM2

Kubernetes **compose les couches r√©seau et applicatives** √† partir de fichiers **YAML d√©claratifs**.
Ces fichiers d√©crivent un **√©tat souhait√©**, que le cluster maintient automatiquement.

Le **namespace isole**,
le **service connecte**,
l‚Äô**ingress expose**,
et la **policy prot√®ge**.

Le **TD4** prolongera cette logique avec :

- Installation de **Traefik** (Ingress Controller)
- Exploration du **r√©seau entre pods**
- Introduction √† **Lens** et **Prometheus**
- Observation en temps r√©el de la charge et du trafic.
