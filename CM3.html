<section id="cm3-bloc1">
  <h2>CM3 ‚Äì Gestion de la durabilit√© et de l‚Äô√©tat dans Kubernetes</h2>

  <h3>Bloc 1 ‚Äî Du d√©ploiement √©ph√©m√®re √† la supervision du cluster</h3>
  <hr />

  <h4>1. Contexte g√©n√©ral : du d√©ploiement √©ph√©m√®re √† la production</h4>
  <p>
    Dans Kubernetes,
    <strong>les conteneurs sont par d√©finition √©ph√©m√®res</strong> : ils peuvent
    √™tre d√©truits et recr√©√©s √† tout moment selon les besoins du cluster. Cette
    caract√©ristique garantit la r√©silience, mais impose de comprendre
    <strong>comment l‚Äô√©tat et la durabilit√© sont g√©r√©s</strong>.
  </p>

  <blockquote>
    <p>
      <strong>Objectif du CM3</strong> : comprendre comment Kubernetes maintient
      la stabilit√©, la sant√© et la persistance des applications, malgr√© la
      nature √©ph√©m√®re des conteneurs.
    </p>
  </blockquote>

  <p>Ce cours aborde :</p>
  <ul>
    <li>la <strong>r√©action du cluster aux crashs</strong> ;</li>
    <li>
      la <strong>surveillance des applications</strong> (readiness, liveness) ;
    </li>
    <li>la <strong>gestion des ressources et de la persistance</strong> ;</li>
    <li>et la <strong>supervision du cluster</strong>.</li>
  </ul>

  <hr />

  <h4>2. Gestion des crashs et de l‚Äô√©tat des pods</h4>
  <h5>2.1 Cycle de vie d‚Äôun Pod</h5>

  <p>
    Un <strong>Pod</strong> encapsule un ou plusieurs conteneurs partageant le
    m√™me r√©seau et stockage. Il traverse plusieurs √©tats :
  </p>

  <table>
    <thead>
      <tr>
        <th>√âtat</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Pending</strong></td>
        <td>
          Le pod est accept√© mais ses conteneurs ne sont pas encore d√©marr√©s.
        </td>
      </tr>
      <tr>
        <td><strong>Running</strong></td>
        <td>Tous les conteneurs sont lanc√©s et fonctionnent.</td>
      </tr>
      <tr>
        <td><strong>Succeeded</strong></td>
        <td>Tous les conteneurs se sont termin√©s sans erreur.</td>
      </tr>
      <tr>
        <td><strong>Failed</strong></td>
        <td>Au moins un conteneur s‚Äôest arr√™t√© avec une erreur.</td>
      </tr>
      <tr>
        <td><strong>CrashLoopBackOff</strong></td>
        <td>Le conteneur √©choue et red√©marre en boucle.</td>
      </tr>
    </tbody>
  </table>

  <pre><code class="bash">kubectl get pods -w
kubectl describe pod &lt;nom&gt;
kubectl logs &lt;nom&gt;</code></pre>

  <pre><code class="bash">NAME           READY   STATUS             RESTARTS   AGE
mailpit-7d89   1/1     Running            0          5m
api-5f86d4c    0/1     CrashLoopBackOff   3          2m</code></pre>

  <h5>2.2 Comportement en cas de crash</h5>
  <ol>
    <li>Le <strong>kubelet</strong> d√©tecte l‚Äô√©chec du conteneur.</li>
    <li>Il consulte la <strong>politique de red√©marrage</strong> :</li>
  </ol>

  <pre><code class="yaml">restartPolicy: Always</code></pre>
  <p>
    Le conteneur est alors relanc√© automatiquement, assurant une
    <strong>auto-gu√©rison basique</strong> du cluster.
  </p>

  <h5>2.3 R√¥le du runtime (Containerd)</h5>
  <p>
    Le kubelet s‚Äôappuie sur le <strong>container runtime</strong> (souvent
    <em>Containerd</em> ou <em>CRI-O</em>) pour :
  </p>
  <ul>
    <li>cr√©er et ex√©cuter les conteneurs ;</li>
    <li>g√©rer le syst√®me de fichiers et le r√©seau ;</li>
    <li>superviser leur √©tat.</li>
  </ul>

  <pre><code class="bash">minikube ssh
sudo ctr containers list</code></pre>

  <p>
    Le conteneur peut subsister temporairement m√™me apr√®s la suppression du pod
    Kubernetes.
  </p>

  <h5>2.4 CrashLoopBackOff</h5>
  <p>
    Cet √©tat indique que Kubernetes tente p√©riodiquement de red√©marrer un
    conteneur en √©chec, avec un d√©lai croissant.
  </p>

  <pre><code class="bash">Warning  BackOff  kubelet  Back-off restarting failed container</code></pre>

  <h5>2.5 Nettoyage et persistance</h5>
  <ul>
    <li>
      Les fichiers internes au conteneur sont <strong>perdus</strong> √† la
      suppression.
    </li>
    <li>
      Les donn√©es mont√©es sur un <strong>volume persistant</strong> (PV) sont
      conserv√©es.
    </li>
  </ul>
  <p>
    <em>Nous verrons dans la suite comment rendre ces donn√©es persistantes.</em>
  </p>

  <hr />

  <h4>2 bis. Sant√© du cluster et supervision des pods syst√®mes</h4>

  <h5>2.6 Namespace <code>kube-system</code></h5>
  <p>
    Ce namespace regroupe les <strong>pods essentiels</strong> du cluster
    (r√©seau, contr√¥le, planification, etc.).
  </p>
  <pre><code class="bash">kubectl get pods -n kube-system</code></pre>

  <h5>2.7 Services vitaux du cluster</h5>
  <table>
    <thead>
      <tr>
        <th>Composant</th>
        <th>R√¥le principal</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>CoreDNS</strong></td>
        <td>R√©solution interne de noms entre pods et services.</td>
      </tr>
      <tr>
        <td><strong>etcd</strong></td>
        <td>Base de donn√©es cl√©-valeur stockant l‚Äô√©tat du cluster.</td>
      </tr>
      <tr>
        <td><strong>kube-apiserver</strong></td>
        <td>
          API du plan de contr√¥le : interface et coh√©rence des donn√©es dans
          etcd.
        </td>
      </tr>
      <tr>
        <td><strong>kube-scheduler</strong></td>
        <td>Affectation des pods aux n≈ìuds disponibles.</td>
      </tr>
      <tr>
        <td><strong>controller-manager</strong></td>
        <td>Application des boucles de r√©conciliation.</td>
      </tr>
      <tr>
        <td><strong>kube-proxy</strong></td>
        <td>Routage du trafic entre services et pods.</td>
      </tr>
      <tr>
        <td><strong>kindnet / calico / flannel</strong></td>
        <td>Gestion du r√©seau inter-pods.</td>
      </tr>
      <tr>
        <td><strong>minikube addons manager</strong></td>
        <td>Gestion des extensions locales.</td>
      </tr>
      <tr>
        <td><strong>kubelet</strong></td>
        <td>Agent ex√©cutant les conteneurs sur chaque n≈ìud.</td>
      </tr>
    </tbody>
  </table>

  <pre><code class="bash">kubectl logs &lt;pod&gt; -n kube-system</code></pre>

  <h5>2.8 Configuration du plan de contr√¥le</h5>
  <p>
    Les pods du plan de contr√¥le sont d√©finis dans
    <code>/etc/kubernetes/manifests</code> sur le n≈ìud ma√Ætre.
  </p>
  <pre><code class="bash">minikube ssh
ls /etc/kubernetes/manifests
cat kube-apiserver.yaml</code></pre>
  <p>
    Si un fichier manifest est supprim√©, le pod dispara√Æt ; s‚Äôil est restaur√©,
    il est recr√©√© automatiquement.
  </p>

  <blockquote>
    <p>
      <strong>Le kubelet</strong> g√®re ces pods statiques localement,
      ind√©pendamment de l‚ÄôAPI server.
    </p>
  </blockquote>

  <h5>2.9 Monitoring du cluster avec un DaemonSet (Glances)</h5>
  <p>
    Les <strong>DaemonSets</strong> d√©ploient un pod sur chaque n≈ìud du cluster,
    utile pour le monitoring.
  </p>

  <pre><code class="yaml">apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: glances
spec:
  selector:
    matchLabels:
      app: glances
  template:
    metadata:
      labels:
        app: glances
    spec:
      containers:
        - name: glances
          image: nicolargo/glances:latest
          ports:
            - containerPort: 61208
          volumeMounts:
            - mountPath: /var/run/docker.sock
              name: docker-sock
      volumes:
        - name: docker-sock
          hostPath:
            path: /var/run/docker.sock</code></pre>

  <pre><code class="bash">kubectl get daemonsets
kubectl get pods -l app=glances -o wide</code></pre>

  <p>
    Pour ex√©cuter Glances sur les n≈ìuds ma√Ætres, ajouter des
    <strong>tolerations</strong>.
  </p>

  <hr />

  <h4>Synth√®se du bloc 1</h4>
  <ul>
    <li>
      Les pods sont √©ph√©m√®res mais Kubernetes assure leur
      <strong>auto-r√©cup√©ration</strong>.
    </li>
    <li>
      Le <strong>kubelet</strong> et <strong>Containerd</strong> pilotent le
      cycle de vie des conteneurs.
    </li>
    <li>
      Le namespace <code>kube-system</code> contient les
      <strong>services vitaux</strong> du cluster.
    </li>
    <li>Les manifests statiques maintiennent le plan de contr√¥le.</li>
    <li>
      Les <strong>DaemonSets</strong> permettent de d√©ployer des agents sur tous
      les n≈ìuds (ex : Glances).
    </li>
  </ul>
</section>
<section id="cm3-bloc2">
  <h3>Bloc 2 ‚Äî Surveillance et gestion des ressources</h3>
  <hr />

  <h4>3. Surveillance de l‚Äôapplication : readiness et liveness probes</h4>

  <h5>3.1 Pourquoi surveiller ?</h5>
  <p>
    Un pod peut √™tre <strong>en cours d‚Äôex√©cution</strong> sans √™tre
    <strong>fonctionnel</strong>. Les probes (ou sondes) permettent √† Kubernetes
    de <strong>tester automatiquement la sant√©</strong> et la disponibilit√© des
    conteneurs.
  </p>

  <p>
    Kubernetes d√©finit <strong>trois types de sondes fonctionnelles</strong> :
  </p>

  <table>
    <thead>
      <tr>
        <th>Type de sonde</th>
        <th>Objectif principal</th>
        <th>Comportement</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Liveness Probe</strong></td>
        <td>V√©rifie si le conteneur est toujours vivant.</td>
        <td>Red√©marre le conteneur s‚Äôil √©choue.</td>
      </tr>
      <tr>
        <td><strong>Readiness Probe</strong></td>
        <td>V√©rifie si le conteneur est pr√™t √† recevoir du trafic.</td>
        <td>Retire le pod du Service jusqu‚Äô√† r√©tablissement.</td>
      </tr>
      <tr>
        <td><strong>Startup Probe</strong></td>
        <td>V√©rifie si le conteneur a fini de d√©marrer.</td>
        <td>Donne plus de temps avant l‚Äôactivation des autres sondes.</td>
      </tr>
    </tbody>
  </table>

  <hr />
  <h5>√Ä propos des chemins <code>/healthz</code> et <code>/ready</code></h5>

  <p>
    Dans Kubernetes, les applications exposent souvent des
    <strong>points d‚Äôentr√©e de supervision</strong> :
  </p>

  <table>
    <thead>
      <tr>
        <th>Endpoint</th>
        <th>Usage</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><code>/healthz</code></td>
        <td>Liveness</td>
        <td>V√©rifie que l‚Äôapplication r√©pond encore.</td>
      </tr>
      <tr>
        <td><code>/ready</code> ou <code>/readiness</code></td>
        <td>Readiness</td>
        <td>V√©rifie que l‚Äôapplication est pr√™te √† recevoir des requ√™tes.</td>
      </tr>
      <tr>
        <td><code>/metrics</code></td>
        <td>Monitoring</td>
        <td>Expose des m√©triques pour Prometheus ou d‚Äôautres outils.</td>
      </tr>
    </tbody>
  </table>

  <blockquote>
    <p>
      Ces endpoints sont libres mais
      <strong>standardis√©s par convention</strong> : Kubernetes n‚Äôimpose pas
      leur nom, mais beaucoup d‚Äôoutils les reconnaissent.
    </p>
  </blockquote>

  <hr />

  <h5>3.2 M√©thodes techniques de probes</h5>
  <p>
    Chaque type de sonde peut utiliser diff√©rentes m√©thodes pour effectuer son
    test :
  </p>

  <table>
    <thead>
      <tr>
        <th>M√©thode technique</th>
        <th>Description</th>
        <th>Exemple</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>HTTP GET</strong></td>
        <td>V√©rifie la r√©ponse d‚Äôun endpoint HTTP.</td>
        <td>/healthz, /ready</td>
      </tr>
      <tr>
        <td><strong>TCP Socket</strong></td>
        <td>Teste la connectivit√© r√©seau sur un port.</td>
        <td>port 8080</td>
      </tr>
      <tr>
        <td><strong>Exec Command</strong></td>
        <td>Ex√©cute une commande locale et v√©rifie le code retour.</td>
        <td>ps aux</td>
      </tr>
    </tbody>
  </table>

  <pre><code class="yaml">livenessProbe:
  httpGet:
    path: /healthz
    port: 8080
  initialDelaySeconds: 10
  periodSeconds: 5

readinessProbe:
  tcpSocket:
    port: 8080
  initialDelaySeconds: 5
  periodSeconds: 5</code></pre>

  <hr />
  <h5>3.3 Bonnes pratiques</h5>
  <ul>
    <li>
      Pr√©voir un
      <strong>d√©lai initial</strong> (<code>initialDelaySeconds</code>) avant le
      premier test.
    </li>
    <li>
      Adapter la fr√©quence (<code>periodSeconds</code>) √† la charge r√©elle.
    </li>
    <li>
      Configurer les seuils <code>failureThreshold</code> et
      <code>successThreshold</code>.
    </li>
    <li>Aligner les endpoints de test sur la logique applicative r√©elle.</li>
  </ul>
  <p>
    <em>Une sonde trop agressive peut red√©marrer inutilement un conteneur.</em>
  </p>

  <hr />
  <h5>3.4 Exemple : application Flask</h5>
  <pre><code class="python">from flask import Flask
app = Flask(__name__)

@app.route('/healthz')
def health():
    return 'OK', 200</code></pre>

  <pre><code class="yaml">containers:
  - name: web
    image: myapp:latest
    ports:
      - containerPort: 5000
    livenessProbe:
      httpGet:
        path: /healthz
        port: 5000
      initialDelaySeconds: 5
      periodSeconds: 5</code></pre>

  <p>
    Si Flask cesse de r√©pondre, Kubernetes red√©marre automatiquement le
    conteneur.
  </p>

  <hr />

  <h4>4. Gestion des ressources : CPU et m√©moire</h4>
  <h5>4.1 Pourquoi limiter les ressources ?</h5>
  <p>
    Kubernetes mutualise les ressources d‚Äôun n≈ìud entre plusieurs pods. D√©finir
    des <strong>requests</strong> et <strong>limits</strong> permet :
  </p>
  <ul>
    <li>d‚Äôassurer une <strong>r√©partition √©quitable</strong> ;</li>
    <li>d‚Äô√©viter la saturation du CPU ou de la RAM ;</li>
    <li>d‚Äôam√©liorer la stabilit√© du cluster.</li>
  </ul>

  <pre><code class="yaml">resources:
  requests:
    cpu: "500m"
    memory: "256Mi"
  limits:
    cpu: "1"
    memory: "512Mi"</code></pre>

  <ul>
    <li><code>requests</code> : quantit√© garantie pour le conteneur.</li>
    <li><code>limits</code> : plafond maximal autoris√©.</li>
  </ul>
  <p><em>1 CPU = 1000 m ; 500 m = 50 % d‚Äôun c≈ìur.</em></p>

  <hr />
  <h5>4.3 Effets des d√©passements</h5>
  <ul>
    <li>D√©passement m√©moire ‚Üí conteneur tu√© (<code>OOMKilled</code>).</li>
    <li>D√©passement CPU ‚Üí conteneur ralenti (throttling).</li>
  </ul>

  <pre><code class="bash">kubectl describe pod &lt;nom&gt;
# Rechercher 'OOMKilled' ou 'Throttling CPU'</code></pre>

  <hr />
  <h5>4.4 R√©servation et surallocation</h5>
  <p>
    Kubernetes permet une <strong>surallocation contr√¥l√©e</strong> : plusieurs
    pods peuvent demander plus que la capacit√© totale du n≈ìud, mais le
    <strong>scheduler</strong>
    priorise ceux ayant les besoins les plus critiques.
  </p>

  <hr />
  <h5>4.5 Priorit√©s des pods</h5>
  <pre><code class="yaml">apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 1000
preemptionPolicy: PreemptLowerPriority</code></pre>

  <pre><code class="yaml">spec:
  priorityClassName: high-priority</code></pre>
  <p>
    Un pod de haute priorit√© peut pr√©empter un autre lorsque les ressources
    manquent.
  </p>

  <hr />
  <h4>Synth√®se du bloc 2</h4>
  <ul>
    <li>Les probes garantissent la sant√© et la disponibilit√© des pods.</li>
    <li>
      Les sondes <strong>liveness</strong>, <strong>readiness</strong> et
      <strong>startup</strong> contr√¥lent la vitalit√© et la pr√©paration.
    </li>
    <li>
      Les <strong>requests</strong> et <strong>limits</strong> encadrent la
      consommation des ressources.
    </li>
    <li>
      Les <strong>PriorityClasses</strong> assurent la continuit√© des services
      essentiels.
    </li>
  </ul>
  <blockquote>
    <p>
      Ces m√©canismes permettent √† Kubernetes d‚Äôassurer la
      <strong>qualit√© de service</strong> (QoS) et la
      <strong>r√©silience applicative</strong> du cluster.
    </p>
  </blockquote>
</section>
<section id="cm3-bloc3">
  <h3>Bloc 3 ‚Äî Persistance des donn√©es et StatefulSets</h3>
  <hr />

  <h4>5. Persistance des donn√©es</h4>

  <h5>5.1 Pourquoi la persistance est-elle n√©cessaire ?</h5>
  <p>
    Les pods Kubernetes sont <strong>√©ph√©m√®res</strong> : lors d‚Äôun red√©marrage,
    leur syst√®me de fichiers est reconstruit √† partir de l‚Äôimage du conteneur.
    Toute donn√©e stock√©e localement est donc <strong>perdue</strong>.
  </p>
  <p>
    Pour les applications manipulant des donn√©es (bases de donn√©es, journaux,
    messagerie, etc.), il faut un moyen de
    <strong>pr√©server ces informations entre les cycles de vie des pods</strong
    >. Kubernetes fournit pour cela un mod√®le de
    <strong>volumes persistants (PV/PVC)</strong>, qui d√©couple la dur√©e de vie
    du stockage de celle des pods.
  </p>

  <hr />
  <h5>5.2 Cr√©ation manuelle : PV + PVC li√©s statiquement</h5>

  <h6>a. √âtape 1 ‚Äî D√©finir un PersistentVolume (PV)</h6>
  <p>
    Un <strong>PersistentVolume</strong> repr√©sente une ressource de stockage
    physique ou logique disponible dans le cluster.
  </p>

  <pre><code class="yaml">apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-demo
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: /data/demo</code></pre>

  <p>
    Le volume est ici stock√© localement sur le n≈ìud dans le r√©pertoire
    <code>/data/demo</code>.
  </p>

  <h6>b. √âtape 2 ‚Äî Cr√©er un PersistentVolumeClaim (PVC)</h6>
  <p>
    Un <strong>PersistentVolumeClaim</strong> (PVC) est la <em>demande</em> d‚Äôun
    pod pour un volume persistant.
  </p>

  <pre><code class="yaml">apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-demo
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi</code></pre>

  <p>
    Lorsque le PVC est cr√©√©, Kubernetes recherche un PV compatible (taille, mode
    d‚Äôacc√®s, disponibilit√©) et le lie automatiquement (<code>Bound</code>).
  </p>

  <pre><code class="bash">kubectl get pv,pvc</code></pre>

  <h6>c. √âtape 3 ‚Äî Utiliser le PVC dans un Pod</h6>
  <pre><code class="yaml">apiVersion: v1
kind: Pod
metadata:
  name: demo-pod
spec:
  containers:
    - name: demo
      image: busybox
      command: ["sleep", "3600"]
      volumeMounts:
        - mountPath: /mnt/data
          name: demo-volume
  volumes:
    - name: demo-volume
      persistentVolumeClaim:
        claimName: pvc-demo</code></pre>

  <p>
    Le pod aura acc√®s au dossier <code>/data/demo</code> du n≈ìud sous
    <code>/mnt/data</code>.
  </p>

  <hr />
  <h5>5.3 Du mod√®le statique au mod√®le dynamique (StorageClass)</h5>

  <p>
    La m√©thode pr√©c√©dente impose de
    <strong>cr√©er manuellement chaque PV</strong> avant de le r√©clamer via un
    PVC, ce qui est peu pratique dans des environnements √©volutifs (multi-n≈ìuds,
    cloud, CI/CD). Kubernetes introduit les <strong>StorageClasses</strong> pour
    automatiser cette cr√©ation.
  </p>

  <h6>a. StorageClass : principe</h6>
  <p>
    Une <strong>StorageClass</strong> d√©finit comment Kubernetes cr√©e et g√®re
    les volumes :
  </p>
  <ul>
    <li>
      choix du <strong>provisioner</strong> (driver CSI, hostPath, NFS, EBS,
      Ceph, etc.),
    </li>
    <li>politique de r√©clamation,</li>
    <li>moment d‚Äôallocation (imm√©diat ou diff√©r√©).</li>
  </ul>

  <pre><code class="yaml">apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: standard
provisioner: k8s.io/minikube-hostpath
reclaimPolicy: Delete
volumeBindingMode: Immediate</code></pre>

  <h6>b. PVC dynamique</h6>
  <p>
    Lorsqu‚Äôun PVC utilise une StorageClass, Kubernetes
    <strong>provisionne automatiquement un PV</strong> adapt√© :
  </p>

  <pre><code class="yaml">apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-dyn
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi
  storageClassName: standard</code></pre>

  <p>Le PV est cr√©√© et li√© dynamiquement au PVC.</p>
  <pre><code class="bash">kubectl get pv,pvc</code></pre>

  <blockquote>
    <p>
      üí° L‚Äôadministrateur ne d√©clare plus de volumes √† l‚Äôavance : Kubernetes
      provisionne √† la vol√©e via la StorageClass.
    </p>
  </blockquote>

  <h6>‚ö†Ô∏è Limite importante du provisionnement dynamique</h6>
  <blockquote>
    <p>
      Le provisionnement dynamique
      <strong>ne garantit pas la portabilit√© du stockage</strong> entre les
      n≈ìuds :
    </p>
    <ul>
      <li>
        Les StorageClasses bas√©es sur un <strong>stockage r√©seau</strong> (NFS,
        CephFS, Longhorn, CSI Cloud, etc.) offrent une accessibilit√© compl√®te.
      </li>
      <li>
        Les StorageClasses bas√©es sur un
        <strong>disque local</strong> (<code>hostPath</code>,
        <code>local</code>) sont limit√©es √† un n≈ìud unique. Kubernetes doit
        alors planifier le pod sur ce n≈ìud (via <code>nodeAffinity</code> ou
        <code>WaitForFirstConsumer</code>).
      </li>
    </ul>
    <p>
      üëâ Le mod√®le dynamique est pertinent uniquement avec un
      <strong>backend r√©seau partag√©</strong>.
    </p>
  </blockquote>

  <h6>c. Comparatif : volumes statiques vs dynamiques</h6>
  <table>
    <thead>
      <tr>
        <th>Aspect</th>
        <th>PV/PVC statiques</th>
        <th>StorageClass dynamique</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>Cr√©ation du volume</td>
        <td>Manuelle (PV d√©fini avant le PVC)</td>
        <td>Automatique</td>
      </tr>
      <tr>
        <td>Association PV‚ÄìPVC</td>
        <td>Bas√©e sur correspondance</td>
        <td>G√©r√©e par le provisioner</td>
      </tr>
      <tr>
        <td>Portabilit√©</td>
        <td>Limit√©e</td>
        <td>Optimale via stockage r√©seau</td>
      </tr>
      <tr>
        <td>Cas d‚Äôusage</td>
        <td>D√©monstration, disque local</td>
        <td>Cluster multi-n≈ìuds, cloud</td>
      </tr>
    </tbody>
  </table>

  <hr />
  <h5>5.4 Modes d‚Äôacc√®s, s√©curit√© et typologie des stockages</h5>

  <h6>a. Modes d‚Äôacc√®s</h6>
  <table>
    <thead>
      <tr>
        <th>Mode</th>
        <th>Signification</th>
        <th>Cas d‚Äôusage</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>ReadWriteOnce (RWO)</strong></td>
        <td>Lecture/√©criture par un seul n≈ìud.</td>
        <td>Disque local, base de donn√©es.</td>
      </tr>
      <tr>
        <td><strong>ReadOnlyMany (ROX)</strong></td>
        <td>Lecture seule par plusieurs n≈ìuds.</td>
        <td>Fichiers statiques, images.</td>
      </tr>
      <tr>
        <td><strong>ReadWriteMany (RWX)</strong></td>
        <td>Lecture/√©criture simultan√©e.</td>
        <td>NFS, CephFS, GlusterFS.</td>
      </tr>
      <tr>
        <td><strong>ReadWriteOncePod (RWOP)</strong></td>
        <td>Acc√®s exclusif par un seul pod.</td>
        <td>S√©curit√© renforc√©e.</td>
      </tr>
    </tbody>
  </table>

  <h6>b. S√©curit√© et permissions</h6>
  <p>
    Les droits POSIX sur le volume mont√© peuvent √™tre d√©finis via un
    <strong>securityContext</strong> :
  </p>
  <pre><code class="yaml">securityContext:
  runAsUser: 1000
  fsGroup: 1000</code></pre>

  <h6>c. Types de backend</h6>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
        <th>Particularit√©s</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>hostPath / local</strong></td>
        <td>Stockage local du n≈ìud.</td>
        <td>Rapide mais non partag√©.</td>
      </tr>
      <tr>
        <td><strong>NFS</strong></td>
        <td>Partage r√©seau simple.</td>
        <td>Compatible RWX.</td>
      </tr>
      <tr>
        <td><strong>CSI Driver</strong></td>
        <td>Interface standard.</td>
        <td>Supporte Ceph, EBS, Azure, etc.</td>
      </tr>
      <tr>
        <td><strong>CephFS / RBD</strong></td>
        <td>Stockage distribu√© en r√©seau.</td>
        <td>Haute disponibilit√©.</td>
      </tr>
      <tr>
        <td><strong>Cloud storage</strong></td>
        <td>EBS, Persistent Disk, etc.</td>
        <td>Provisionnement dynamique complet.</td>
      </tr>
    </tbody>
  </table>

  <hr />
  <h4>6. StatefulSets et bases de donn√©es</h4>

  <h5>6.1 Limites des Deployments</h5>
  <p>
    Les <strong>Deployments</strong> conviennent aux applications
    <em>stateless</em>. Pour les bases n√©cessitant une identit√© stable, on
    utilise un <strong>StatefulSet</strong>.
  </p>
  <blockquote>
    <p>
      ‚ö†Ô∏è Kubernetes ne g√®re pas la coh√©rence applicative (r√©plication,
      concurrence, transactions) : cela rel√®ve du moteur de base (MySQL,
      PostgreSQL, MongoDB, etc.).
    </p>
  </blockquote>

  <h5>6.2 Caract√©ristiques d‚Äôun StatefulSet</h5>
  <table>
    <thead>
      <tr>
        <th>Fonction</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>Identit√© stable</td>
        <td>
          Pods nomm√©s s√©quentiellement (<code>app-0</code>,
          <code>app-1</code>...)
        </td>
      </tr>
      <tr>
        <td>Volume d√©di√©</td>
        <td>Chaque pod poss√®de son PVC personnel.</td>
      </tr>
      <tr>
        <td>Ordre contr√¥l√©</td>
        <td>Cr√©ation et suppression ordonn√©es.</td>
      </tr>
      <tr>
        <td>Stabilit√©</td>
        <td>Le nom et le volume persistent apr√®s red√©marrage.</td>
      </tr>
    </tbody>
  </table>

  <h5>6.3 Exemple : MariaDB avec StatefulSet</h5>
  <pre><code class="yaml">apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mariadb
spec:
  selector:
    matchLabels:
      app: mariadb
  serviceName: mariadb
  replicas: 1
  template:
    metadata:
      labels:
        app: mariadb
    spec:
      containers:
        - name: mariadb
          image: mariadb:10.11
          env:
            - name: MYSQL_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: mariadb-secret
                  key: root-password
          ports:
            - containerPort: 3306
          volumeMounts:
            - name: data
              mountPath: /var/lib/mysql
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes: ["ReadWriteOnce"]
        resources:
          requests:
            storage: 2Gi</code></pre>

  <ul>
    <li>
      <code>volumeClaimTemplates</code> cr√©e un PVC par pod
      (<code>data-mariadb-0</code>, etc.).
    </li>
    <li>
      Si <code>replicas &gt; 1</code>, les instances sont ind√©pendantes sans
      r√©plication configur√©e.
    </li>
    <li>
      Pour la coh√©rence : utiliser Galera, Group Replication, Service Headless
      (<code>clusterIP: None</code>).
    </li>
  </ul>

  <pre><code class="bash">kubectl get statefulsets
kubectl get pods -l app=mariadb
kubectl get pvc | grep mariadb</code></pre>

  <pre><code class="bash">kubectl scale statefulset mariadb --replicas=3</code></pre>

  <blockquote>
    <p>
      Kubernetes orchestre les pods et volumes, mais pas la logique
      transactionnelle.
    </p>
  </blockquote>

  <h5>6.5 ConfigMaps et Secrets</h5>
  <pre><code class="yaml">apiVersion: v1
kind: Secret
metadata:
  name: mariadb-secret
type: Opaque
data:
  root-password: bXlzZWNyZXRwYXNzCg== # base64("mysecretpass")</code></pre>
  <p>
    Les <strong>Secrets</strong> et <strong>ConfigMaps</strong> stockent la
    configuration et les mots de passe de mani√®re s√©curis√©e. Ils sont
    <em>namespaced</em> et non persistants, mais li√©s au d√©ploiement.
  </p>

  <hr />
  <h4>üß† Synth√®se du bloc 3</h4>
  <ul>
    <li>Les <strong>PV/PVC</strong> assurent la persistance des donn√©es.</li>
    <li>
      Les <strong>StorageClasses</strong> permettent le provisionnement
      dynamique.
    </li>
    <li>
      Ce mod√®le est efficace avec un
      <strong>stockage en r√©seau partag√©</strong>.
    </li>
    <li>
      Les modes d‚Äôacc√®s, droits et backends d√©terminent la souplesse du
      stockage.
    </li>
    <li>
      Les <strong>StatefulSets</strong> g√®rent les applications avec √©tat, mais
      la coh√©rence reste applicative.
    </li>
  </ul>
  <blockquote>
    <p>
      üí° Kubernetes orchestre la <strong>durabilit√©</strong>, la
      <strong>stabilit√©</strong> et la <strong>persistance</strong> des
      applications distribu√©es.
    </p>
  </blockquote>
</section>
