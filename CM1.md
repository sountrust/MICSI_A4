# ğŸ“˜ Table des matiÃ¨res

- [1ï¸âƒ£ â€“ Du monolithe aux microservices](#1ï¸âƒ£--du-monolithe-aux-microservices)
- [2ï¸âƒ£ â€“ Virtualisation : lâ€™isolation matÃ©rielle](#2ï¸âƒ£--virtualisation--lisolation-matÃ©rielle)
- [3ï¸âƒ£ â€“ Conteneurisation : lâ€™isolation logique](#3ï¸âƒ£--conteneurisation--lisolation-logique)
- [4ï¸âƒ£ â€“ Kubernetes : orchestrer les conteneurs virtualisÃ©s](#4ï¸âƒ£--kubernetes--orchestrer-les-conteneurs-virtualisÃ©s)
- [5ï¸âƒ£ â€“ La virtualisation au service de lâ€™orchestration](#5ï¸âƒ£--la-virtualisation-au-service-de-lorchestration)

---

# 1ï¸âƒ£ â€“ Du monolithe aux microservices

> **Une application monolithique** regroupe toutes les fonctionnalitÃ©s dans un seul programme.

---

## âš™ï¸ CaractÃ©ristiques

- Une **base de code unique**, un seul processus, un seul cycle de dÃ©ploiement.
- SimplicitÃ© initiale âœ… mais forte **dÃ©pendance interne** âŒ entre les modules.
- Tout changement ou panne impacte **lâ€™ensemble du systÃ¨me**.

ğŸ“š **Lien recommandÃ© :**  
[BD Kubernetes par Google Cloud](https://cloud.google.com/kubernetes-engine/kubernetes-comic/)

---

## ğŸ•°ï¸ Historique et contexte dâ€™Ã©volution

Dans les annÃ©es **1990â€“2000**, la majoritÃ© des applications dâ€™entreprise Ã©taient **monolithiques** :

- Architecture **client-serveur**
- Mises Ã  jour nÃ©cessitant lâ€™arrÃªt complet du service ğŸ›‘
- ScalabilitÃ© **verticale** (plus de matÃ©riel)

---

### ğŸš€ Les causes de lâ€™Ã©volution

- **Complexification** des systÃ¨mes
- **Ã‰mergence du web** et besoin dâ€™intÃ©gration
- Nouveaux modÃ¨les **DevOps / CI/CD**

---

### ğŸŒ Les premiÃ¨res interconnexions

| Technologie | AnnÃ©e | Description                                  |
| ----------- | ----- | -------------------------------------------- |
| **SOAP**    | 1999  | Web Services XML (interopÃ©rabilitÃ© initiale) |
| **REST**    | 2000  | Communication simple HTTP (JSON / XML)       |
| **gRPC**    | 2015  | Protocole binaire performant basÃ© sur HTTP/2 |

ğŸ’¬ Ces standards ont permis la communication entre modules indÃ©pendants, amorÃ§ant la **transition vers les microservices**.

---

## ğŸ” Quâ€™est-ce quâ€™un microservice ?

Un **microservice** = une **unitÃ© fonctionnelle autonome** dâ€™une application.

ğŸ§© Il :

- ImplÃ©mente une **fonction mÃ©tier unique** (ex : facturation, loginâ€¦)
- Sâ€™exÃ©cute **indÃ©pendamment**
- PossÃ¨de son **cycle de vie propre**

---

### ğŸŒ‰ Communication

Les microservices Ã©changent via des **APIs lÃ©gÃ¨res**, favorisant :

- la **modularitÃ©** du code ğŸ§ 
- la **tolÃ©rance aux pannes** âš¡
- la **scalabilitÃ© horizontale** ğŸ“ˆ

> âš ï¸ Mais cette libertÃ© ajoute une **complexitÃ© dâ€™infrastructure** : rÃ©seau, monitoring, orchestrationâ€¦

---

## ğŸ³ Microservice vs Conteneur

| âŒ Mythe                       | âœ”ï¸ RÃ©alitÃ©                                                                                     |
| ------------------------------ | ---------------------------------------------------------------------------------------------- |
| Un microservice = un conteneur | Le microservice est **une idÃ©e logicielle**, le conteneur est **un environnement dâ€™exÃ©cution** |

---

### ğŸ§  En rÃ©sumÃ© :

- **Microservice** â†’ concept fonctionnel
- **Conteneur** â†’ mÃ©canisme technique

ğŸ’¡ Un conteneur **hÃ©berge souvent** un microservice :

- Inclut code + dÃ©pendances + runtime
- Assure **cohÃ©rence** entre environnements
- Garantit **immutabilitÃ©** et **interopÃ©rabilitÃ©**

---

## ğŸš¢ Pourquoi la conteneurisation est essentielle

La conteneurisation rÃ©pond aux limites du dÃ©ploiement manuel.

âœ… Avantages :

- Environnement **portable, standardisÃ©, isolÃ©**
- **Remplacement** plutÃ´t que modification
- **InteropÃ©rabilitÃ©** multi-plateforme
- **DÃ©ploiement simplifiÃ©** et **rÃ©silient**

ğŸ’¡ Ces propriÃ©tÃ©s â€” _immutabilitÃ©_ et _interopÃ©rabilitÃ©_ â€” sont la base du **cloud-native** orchestrÃ© par **Kubernetes**.

---

## âš ï¸ Limites du modÃ¨le monolithique

- DifficultÃ© dâ€™Ã©volution et de correction
- ScalabilitÃ© **verticale uniquement**
- DÃ©ploiement **lent et risquÃ©**
- **Couplage fort** entre Ã©quipes et technologies

Exemple :

```bash
java -jar application-complete.jar
```

> ğŸ§± Un seul binaire contenant API, UI, logique mÃ©tier et donnÃ©es.

---

## ğŸ§© Vers la modularitÃ© : lâ€™idÃ©e des microservices

- Chaque service = **code + dÃ©pendances + base de donnÃ©es**
- Communication via **API (HTTP, gRPC, message bus)**
- **ScalabilitÃ© horizontale** ciblÃ©e
- **CI/CD** facilitÃ© ğŸ¯

---

## ğŸ§  Vue architecturale

| Aspect            | ğŸ§± Monolithe  | ğŸ§© Microservices     |
| ----------------- | ------------- | -------------------- |
| Couplage          | Fort ğŸ”—       | Faible ğŸ”“            |
| DÃ©ploiement       | Unique        | IndÃ©pendant          |
| ScalabilitÃ©       | Verticale     | Horizontale          |
| RÃ©silience        | Panne globale | Isolement des pannes |
| ComplexitÃ© rÃ©seau | Faible        | Ã‰levÃ©e âš™ï¸            |

> ğŸ‘‰ Les microservices dÃ©placent la complexitÃ© **du code vers lâ€™infrastructure**.

---

## ğŸ§° ProblÃ¨me nouveau : lâ€™exÃ©cution de tous ces services

Chaque microservice doit :

- ÃŠtre **isolÃ©** de maniÃ¨re fiable ğŸ§³
- **Communiquer** avec les autres services ğŸŒ
- ÃŠtre **mis Ã  jour** sans perturber le reste â™»ï¸

â¡ï¸ Cela demande un **mÃ©canisme dâ€™isolation et de gestion** :

- **Virtualisation** pour sÃ©parer les environnements ğŸ’»
- **Conteneurisation** pour isoler les processus ğŸ§±

Les deux sont **complÃ©mentaires** :

- Virtualisation â†’ base matÃ©rielle âš™ï¸
- Conteneurisation â†’ flexibilitÃ© logicielle ğŸ§©

---

## ğŸ’¡ Exemple de transition pratique

```bash
# Monolithe initial
java -jar monolith.jar

# Microservice isolÃ©
python3 -m http.server 8080
```

> Le service devient indÃ©pendant, mais pour en gÃ©rer **des dizaines ou centaines**, il faut les **isoler**, les **connecter** et les **orchestrer**.

â¡ï¸ Ce besoin mÃ¨nera naturellement vers la **virtualisation et la conteneurisation**.

---

# 2ï¸âƒ£ â€“ Virtualisation : lâ€™isolation matÃ©rielle

> La virtualisation permet dâ€™exÃ©cuter plusieurs environnements sur une mÃªme machine physique.

## ğŸ” DÃ©finition

La **virtualisation** crÃ©e plusieurs **machines virtuelles (VM)** Ã  partir de ressources physiques :

- Chaque VM possÃ¨de son propre **OS**, mÃ©moire, stockage, rÃ©seau.
- Un **hyperviseur** gÃ¨re la rÃ©partition des ressources.

---

## ğŸ§© Types dâ€™hyperviseurs

### Type 1 â€” _Bare Metal_

- Fonctionne **directement sur le matÃ©riel**.
- Haute performance et fiabilitÃ©.
- UtilisÃ© dans les **data centers**.

> Exemples : VMware ESXi, Hyper-V, KVM, Xen.

### Type 2 â€” _HÃ©bergÃ©_

- Fonctionne **au-dessus dâ€™un OS hÃ´te**.
- SimplicitÃ© dâ€™installation.
- IdÃ©al pour **tests ou postes de travail**.

> Exemples : VirtualBox, VMware Workstation, Parallels.

---

## ğŸ§  RÃ´le de lâ€™hyperviseur

- Alloue dynamiquement les ressources ğŸ’¾
- Isole les environnements ğŸ”’
- AgrÃ¨ge ou fractionne le matÃ©riel selon les besoins âš™ï¸

```
MatÃ©riel physique (CPU, RAM, disque)
   â†“
Hyperviseur
   â†“ â†“ â†“
VM1 (Linux) | VM2 (Windows) | VM3 (Ubuntu)
```

---

## âœ… Avantages de la virtualisation

- Isolation complÃ¨te ğŸ§±
- Mutualisation du matÃ©riel ğŸ’°
- PortabilitÃ© ğŸ§³
- FlexibilitÃ© ğŸ§ 
- Abstraction matÃ©rielle ğŸ”Œ

ğŸ’¡ Exemple : un serveur physique hÃ©berge plusieurs VMs (DB, web, stockage).

---

## âš ï¸ Limites

- **Surcharge mÃ©moire** (chaque VM a son OS)
- **DÃ©marrage lent** ğŸ¢
- **Gestion complexe** âš™ï¸

â¡ï¸ Naissance de lâ€™**Infrastructure as Code (IaC)** ğŸ’»

---

## âš™ï¸ Infrastructure as Code (IaC)

> Lâ€™IaC dÃ©crit lâ€™infrastructure comme du **code dÃ©claratif**.

- DÃ©crit lâ€™**Ã©tat attendu** (VMs, rÃ©seaux, services)
- Automatisation de la **crÃ©ation et configuration**
- Facilite **versionnage, reproductibilitÃ©, CI/CD**

ğŸ§° **Outils IaC :** Terraform, OpenTofu, Ansible, Puppet, Chef, CloudFormation, Pulumi.

---

## ğŸ³ De la virtualisation Ã  la conteneurisation

> La conteneurisation **ne remplace pas** la virtualisation, elle sâ€™appuie dessus.

- Les **VMs** assurent lâ€™isolation matÃ©rielle ğŸ”’
- Les **conteneurs** assurent lâ€™isolation logicielle ğŸ§©

ğŸ’¡ Kubernetes combine la **robustesse** des VMs et la **lÃ©gÃ¨retÃ©** des conteneurs.

---

# 3ï¸âƒ£ â€“ Conteneurisation : lâ€™isolation logique

> **But** â€” Comprendre comment la conteneurisation isole les processus applicatifs dans un mÃªme systÃ¨me dâ€™exploitation, prÃ©pare la modularisation des applications et introduit la logique dâ€™orchestration.

---

## ğŸ§  DÃ©finition & objectifs

Un **conteneur** = un **processus isolÃ©** + son environnement dâ€™exÃ©cution minimal (bibliothÃ¨ques, configuration, dÃ©pendances).  
Contrairement Ã  une VM, il **partage le noyau** du systÃ¨me hÃ´te, ce qui le rend **lÃ©ger** et **rapide**.

ğŸ¯ Objectifs principaux :

- **PortabilitÃ©** â†’ un mÃªme conteneur fonctionne sur tout hÃ´te compatible.
- **ImmutabilitÃ©** â†’ le conteneur ne change pas : on le reconstruit plutÃ´t que le modifier.
- **RapiditÃ©** â†’ dÃ©marrage en secondes.
- **DensitÃ©** â†’ plusieurs conteneurs peuvent cohabiter sur la mÃªme machine.

---

## ğŸ§© MÃ©canismes Linux

Les conteneurs reposent sur des fonctionnalitÃ©s natives du noyau Linux :

- **Namespaces** â†’ isolent les espaces dâ€™exÃ©cution :
  - `pid` (processus), `net` (rÃ©seau), `mnt` (systÃ¨me de fichiers), `uts` (nom dâ€™hÃ´te), `ipc`, `user`.
- **cgroups** â†’ contrÃ´lent les ressources CPU, mÃ©moire, I/O, etc.
- **UnionFS / OverlayFS** â†’ superposent les couches de fichiers (lecture seule + overlay dâ€™Ã©criture).
- **Capabilities / seccomp / AppArmor** â†’ restreignent les permissions et appels systÃ¨mes.

ğŸ’¡ Ces mÃ©canismes sont transparents Ã  lâ€™utilisateur : Docker, Podman ou containerd les utilisent sous le capot.

---

## ğŸ§± Image et exÃ©cution

Une **image de conteneur** contient :

- le code de lâ€™application,
- ses dÃ©pendances,
- un systÃ¨me minimal (souvent basÃ© sur Debian, Alpine, Distroless),
- et un point dâ€™entrÃ©e (`ENTRYPOINT`).

```bash
# Exemple simple : image NGINX
sudo docker run -d -p 8080:80 nginx:1.25
curl http://localhost:8080
```

â¡ï¸ Le conteneur expose un service HTTP isolÃ©, sans affecter lâ€™hÃ´te.

---

## ğŸ§° Dockerfile : construction dâ€™une image

```dockerfile
# Exemple : application minimale Node.js
FROM node:20-slim
WORKDIR /app
COPY package*.json ./
RUN npm install --only=production
COPY . .
EXPOSE 5000
CMD ["npm", "start"]
```

Commandes associÃ©es :

```bash
# Construction de lâ€™image
docker build -t myapp:1.0 .

# Lancement du conteneur
docker run -d -p 5000:5000 myapp:1.0
```

> Lâ€™image devient un **artefact versionnÃ©** et partageable sur un registre (Docker Hub, GitLab Registry, GHCRâ€¦).

---

## âš™ï¸ Composition de services (Docker Compose)

Quand plusieurs conteneurs doivent collaborer (ex. application + base de donnÃ©es), on utilise un **fichier de composition** (`docker-compose.yml`).

```yaml
version: "3.9"
services:
  web:
    build: .
    ports:
      - "5000:5000"
    depends_on:
      - db
    environment:
      DATABASE_URL: postgres://app:example@db:5432/appdb
  db:
    image: postgres:15
    environment:
      POSTGRES_DB: appdb
      POSTGRES_USER: app
      POSTGRES_PASSWORD: example
    volumes:
      - data:/var/lib/postgresql/data
volumes:
  data:
```

Commandes principales :

```bash
docker compose up -d --build
docker compose ps
docker compose logs -f
```

ğŸ§© Compose introduit la **dÃ©claration dâ€™un Ã©tat attendu** (dÃ©jÃ  une approche â€œdÃ©clarativeâ€) et la **gestion de dÃ©pendances entre services**.

---

## ğŸ”’ Bonnes pratiques (conteneurs en production)

- Utiliser des **images minimales** (`alpine`, `distroless`).
- Ã‰viter lâ€™exÃ©cution en `root` dans le conteneur.
- Externaliser la configuration (fichiers `.env`, variables dâ€™environnement).
- DÃ©finir des **volumes** pour la persistance (pas Ã©crire dans lâ€™image).
- Versionner les images et les pousser dans un registre privÃ© ou public.

---

## ğŸ§© Transition vers lâ€™orchestration

Lorsque le nombre de conteneurs augmente :

- Il devient nÃ©cessaire de gÃ©rer leur **cycle de vie**, leur **rÃ©seau** et leur **mise Ã  jour**.
- Docker Compose atteint ses limites pour les clusters multi-hÃ´tes.

â¡ï¸ Câ€™est ce besoin qui mÃ¨ne Ã  des orchestrateurs comme **Kubernetes**.

---

## ğŸ” ParallÃ¨le Docker Compose â†”ï¸ Manifeste Kubernetes

| Concept                       | Docker Compose                          | Kubernetes                  |
| ----------------------------- | --------------------------------------- | --------------------------- |
| **Service**                   | DÃ©finit un conteneur et ses dÃ©pendances | `Pod` / `Deployment`        |
| **Port mapping**              | `ports:`                                | `containerPort` / `Service` |
| **Volumes**                   | `volumes:`                              | `PersistentVolumeClaim`     |
| **Variables dâ€™environnement** | `environment:`                          | `env:`                      |
| **RÃ©seau**                    | `bridge` interne                        | `CNI` (rÃ©seau de cluster)   |
| **Fichier**                   | `docker-compose.yml`                    | `manifestes YAML`           |

ğŸ’¡ Kubernetes gÃ©nÃ©ralise et distribue les concepts de Compose Ã  grande Ã©chelle (cluster multi-nÅ“uds, haute disponibilitÃ©, auto-guÃ©rison).

---

## ğŸ§­ Ã€ retenir

- Un conteneur isole un **processus** dans un **mÃªme noyau Linux**.
- Docker et Podman exploitent des mÃ©canismes systÃ¨me (namespaces, cgroups, overlayfs).
- Docker Compose introduit une premiÃ¨re **dÃ©claration dâ€™infrastructure applicative**.
- La montÃ©e en complexitÃ© des environnements distribuÃ©s conduit naturellement Ã  **Kubernetes**, qui orchestre ces conteneurs Ã  lâ€™Ã©chelle du cluster.

---

# 4ï¸âƒ£ â€“ Kubernetes : orchestrer les conteneurs virtualisÃ©s

Quand plusieurs conteneurs doivent coopÃ©rer :

- Automatiser les dÃ©ploiements âš™ï¸
- GÃ©rer le rÃ©seau et les dÃ©pendances ğŸŒ
- Assurer la tolÃ©rance aux pannes ğŸ’ª
- Monter en charge ğŸ“ˆ

â¡ï¸ Apparition des **orchestrateurs** : Docker Swarm, Mesos, **Kubernetes** ğŸš€

## Orchestration & Kubernetes â€” "Ã©tat dÃ©sirÃ©" et rÃ©conciliation

> **Objectif** â€” Comprendre comment Kubernetes orchestre des applications conteneurisÃ©es en appliquant un **modÃ¨le dÃ©claratif** ("Ã©tat dÃ©sirÃ©") et des **boucles de rÃ©conciliation**. DÃ©couvrir les **objets clÃ©s** (Pod, Deployment, Service, Ingress) et la mÃ©canique d'auto-rÃ©tablissement (_selfâ€‘healing_).

---

## ğŸ¯ RÃ©sultats d'apprentissage

- Expliquer le principe **dÃ©claratif** : on dÃ©crit _ce qu'on veut_, pas _comment le faire_.
- DÃ©crire le cycle **rÃ©conciliation â†’ action â†’ observation** dans Kubernetes.
- Identifier les **composants** : API Server, etcd, Scheduler, Controllers, Kubelet, Runtime.
- Lire/Ã©crire des **manifestes YAML** pour Pods / Deployments / Services / Ingress.
- Mettre Ã  l'Ã©chelle (scaling) et comprendre l'**autoâ€‘guÃ©rison** (remplacement de Pods).

---

## ğŸ§  DÃ©claratif vs impÃ©ratif

- **ImpÃ©ratif** : "exÃ©cute ces commandes dans cet ordre" â†’ fragile, non idempotent.
- **DÃ©claratif** : "voici **l'Ã©tat dÃ©sirÃ©** du systÃ¨me" â†’ le contrÃ´leur converge vers cet Ã©tat.

> **ParallÃ¨le IaC** : Terraform/Ansible dÃ©crivent l'infra ; **Kubernetes** dÃ©crit l'Ã©tat applicatif (et rÃ©seau/stockage associÃ©s) au niveau **service**.

---

## ğŸ” Boucle de rÃ©conciliation (vue systÃ¨me)

```mermaid
flowchart LR
  subgraph User[DÃ©veloppeur]
    A["Manifeste YAML : Ã©tat dÃ©sirÃ©"]
  end
  A -->|"kubectl apply"| B["API Server"]
  B --> C["etcd (stocke Ã©tat dÃ©sirÃ©)"]
  B --> D[Controllers]
  D --> E{Ã‰tat rÃ©el conforme ?}
  E -- "Non" --> F["CrÃ©er / Remplacer / Scaler Pods"]
  F --> G["Kubelet sur Nodes"]
  G --> H["Containers (containerd / CRI-O)"]
  H --> I["Ã‰tats et Ã©vÃ©nements"]
  I --> D
  E -- "Oui" --> J[Convergence]

```

---

## ğŸ§± Objets fondamentaux

- **Pod** : plus petite unitÃ© dÃ©ployable (un ou plusieurs conteneurs + rÃ©seau/volumes partagÃ©s).
- **ReplicaSet** : garantit _n_ rÃ©plicas identiques d'un Pod (gÃ©nÃ©rÃ© par un Deployment).
- **Deployment** : stratÃ©gie de mise Ã  jour (rolling update), historique, rollback.
- **Service** : point d'accÃ¨s rÃ©seau stable vers un ensemble de Pods (ClusterIP / NodePort / LoadBalancer).
- **Ingress** : rÃ¨gles HTTP(S) vers des Services (via un _Ingress Controller_ â€” ex. Traefik).
- **Namespace** : cloisonnement logique (quotas, RBAC, isolation).
- **ConfigMap/Secret** : configuration externe & donnÃ©es sensibles.

---

## ğŸ“„ Pod minimal (lecture seule)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: demo-pod
  labels: { app: demo }
spec:
  containers:
    - name: web
      image: nginx:1.25
      ports: [{ containerPort: 80 }]
      resources:
        requests: { cpu: "100m", memory: "64Mi" }
        limits: { cpu: "300m", memory: "128Mi" }
      readinessProbe:
        httpGet: { path: "/", port: 80 }
        initialDelaySeconds: 3
        periodSeconds: 5
```

---

## ğŸ“¦ Deployment (Ã©tat dÃ©sirÃ© rÃ©plicas=3)

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-deploy
  labels: { app: web }
spec:
  replicas: 3
  selector: { matchLabels: { app: web } }
  strategy:
    type: RollingUpdate
    rollingUpdate: { maxUnavailable: 1, maxSurge: 1 }
  template:
    metadata: { labels: { app: web } }
    spec:
      containers:
        - name: web
          image: nginx:1.25
          ports: [{ containerPort: 80 }]
          resources:
            requests: { cpu: "100m", memory: "64Mi" }
            limits: { cpu: "300m", memory: "128Mi" }
          livenessProbe:
            httpGet: { path: "/", port: 80 }
            initialDelaySeconds: 5
            periodSeconds: 10
```

---

## ğŸŒ Service + Ingress (exposition HTTP locale)

```yaml
apiVersion: v1
kind: Service
metadata:
  name: web-svc
spec:
  type: ClusterIP
  selector: { app: web }
  ports:
    - port: 80
      targetPort: 80
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: web-ing
  annotations:
    kubernetes.io/ingress.class: traefik
spec:
  rules:
    - host: web.local
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: web-svc
                port:
                  number: 80
```

# 5ï¸âƒ£ â€“ La virtualisation au service de lâ€™orchestration

> **Objectif** â€” Comprendre comment la virtualisation soutient les mÃ©canismes dâ€™orchestration des conteneurs et pourquoi Kubernetes repose encore sur elle pour garantir isolation, Ã©lasticitÃ© et abstraction des ressources.

---

## âš™ï¸ Virtualisation et orchestration : une relation de dÃ©pendance

Kubernetes ne remplace pas la virtualisation â€” il **sâ€™appuie** dessus.

- La **virtualisation** fournit le **socle dâ€™isolation matÃ©rielle** : chaque nÅ“ud du cluster (control plane ou worker) tourne souvent sur une **machine virtuelle (VM)**.
- Elle permet la **gestion des ressources physiques** : CPU, RAM, disque, rÃ©seau.
- Elle offre la **flexibilitÃ©** nÃ©cessaire Ã  lâ€™orchestrateur pour :
  - crÃ©er ou supprimer des nÅ“uds selon la charge,
  - migrer des workloads,
  - allouer dynamiquement les ressources.

---

## ğŸ’» Exemple selon les environnements

- **Sur un laptop** : `MicroK8s` ou `Minikube` virtualisent implicitement les composants Kubernetes (API Server, Scheduler, kubeletâ€¦) dans des VM ou conteneurs isolÃ©s.
- **Sur un cloud provider** : Kubernetes planifie les Pods sur des VM orchestrÃ©es par le fournisseur (AWS EC2, GCP Compute Engine, Azure VM, OpenStackâ€¦).

```mermaid
flowchart TB
  subgraph Infra[Infrastructure physique]
    A1[Serveurs physiques]
  end
  subgraph Virt[Couche de virtualisation]
    V1[VMs : Nodes du cluster]
  end
  subgraph K8s[Cluster Kubernetes]
    N1[Pods / Deployments / Services]
  end
  A1 --> V1 --> N1
```

â¡ï¸ **Virtualisation** = fondation matÃ©rielle abstraite.  
â¡ï¸ **Conteneurisation** = unitÃ© dâ€™exÃ©cution logique.  
â¡ï¸ **Orchestration (K8s)** = pilotage global et automatisÃ©.

---

## ğŸ§  ComplÃ©mentaritÃ© des couches

| Niveau          | Technologie      | RÃ´le principal                                | Exemple                 |
| --------------- | ---------------- | --------------------------------------------- | ----------------------- |
| **MatÃ©riel**    | Virtualisation   | Isolation des OS et gestion CPU/RAM           | KVM, VMware, Hyperâ€‘V    |
| **SystÃ¨me**     | Conteneurisation | Isolation des processus applicatifs           | Docker, LXC, containerd |
| **Application** | Orchestration    | Gestion de lâ€™Ã©tat dÃ©sirÃ©, scaling, rÃ©silience | Kubernetes              |

---

## ğŸŒ Lâ€™Ã©lasticitÃ© grÃ¢ce Ã  la virtualisation

La virtualisation permet Ã  Kubernetes dâ€™Ãªtre **Ã©lastique** et **rÃ©silient** :

- Ajout ou suppression automatique de **nÅ“uds virtuels** selon la charge.
- **Migration Ã  chaud** possible sur certaines plateformes.
- RÃ©partition des ressources matÃ©rielles sans redÃ©ployer tout le cluster.

> Kubernetes exploite ces capacitÃ©s pour **autoâ€‘scaler** horizontalement ses nÅ“uds ou pods selon la demande.

---

## ğŸ” Conclusion scientifique

- La **virtualisation** opÃ¨re au **niveau de lâ€™infrastructure** : dÃ©couple le matÃ©riel du logiciel.
- La **conteneurisation** opÃ¨re au **niveau du processus** : isole les applications et leurs dÃ©pendances.
- Lâ€™**orchestration** opÃ¨re au **niveau du systÃ¨me applicatif** : dÃ©crit et maintient un Ã©tat dÃ©sirÃ©.

> ğŸ§© Les trois couches sont **interdÃ©pendantes** et forment la base du cloudâ€‘native :  
> Virtualisation â†’ Conteneurisation â†’ Orchestration.

---

## ğŸ—ï¸ HiÃ©rarchie de clusters : du laptop Ã  la production

> **But** â€” Donner aux Ã©tudiants une grille de lecture : _ce quâ€™ils manipulent en TD avec MicroK8s_ vs _ce quâ€™une Ã©quipe opÃ¨re en production_.

### 1) Paliers dâ€™Ã©volution

- **P0 â€” Dev local (laptop)** : MicroK8s/Minikube, 1 nÅ“ud, stockage local, Ingress simple.
- **P1 â€” Singleâ€‘cluster nonâ€‘critique** : 3 nÅ“uds (VM), Control Plane gÃ©rÃ©, StorageClass basique, Ingress HA.
- **P2 â€” HA intraâ€‘rÃ©gion** : 3+ nÅ“uds workers, Control Plane redondÃ©, CSI managÃ©, HPA+Cluster Autoscaler, Registry privÃ©.
- **P3 â€” Multiâ€‘clusters / multiâ€‘rÃ©gions** : fÃ©dÃ©ration logique, DR/BCP, politiques rÃ©seau et sÃ©curitÃ© interâ€‘clusters, GitOps global.

### 2) Ce que MicroK8s masque vs ce quâ€™on gÃ¨re en prod

| Domaine           | En TD (MicroK8s)              | En production                                                |
| ----------------- | ----------------------------- | ------------------------------------------------------------ |
| **Control Plane** | MononÅ“ud, composants packagÃ©s | Plans de contrÃ´le HA / gÃ©rÃ©s (managed K8s)                   |
| **Workers**       | 1 VM / machine                | Pools de nÅ“uds, types de VM, autoscaling                     |
| **RÃ©seau (CNI)**  | CNI par dÃ©faut                | CNI choisi (Calico, Ciliumâ€¦), **NetworkPolicy**              |
| **Stockage**      | stockage local / hostpath     | **CSI** managÃ© (RWO/RWX), classes, snapshots                 |
| **Ingress**       | Ingress simple                | Ingress controller HA (Traefik/Nginx), LB, WAF               |
| **Certificats**   | autoâ€‘signÃ© / non utilisÃ©      | certâ€‘manager, ACME, PKI interne                              |
| **SÃ©curitÃ©**      | par dÃ©faut                    | RBAC fin, **PSA/PodSecurity**, Secrets KMS, image policies   |
| **Images**        | tirÃ©es du hub public          | registres privÃ©s, **ImagePullSecrets**, scannage, signatures |
| **ObservabilitÃ©** | minimale                      | Prometheus/Grafana, logs centralisÃ©s, traces, alerting       |
| **DÃ©ploiements**  | kubectl apply                 | **GitOps** (Flux/Argo), releases Helm, approvals             |

### 3) Topologie visuelle

```mermaid
flowchart TB
  subgraph Dev["P0 â€” Dev local"]
    L["MicroK8s (1 node)"]
  end

  subgraph P1["P1 â€” Single-cluster"]
    CP1["Control Plane gÃ©rÃ©"]
    NP1["Node Pool"]
    CP1 --> NP1
  end

  subgraph P2["P2 â€” HA intra-rÃ©gion"]
    CP2["Control Plane HA"]
    NP2a["Pool Compute"]
    NP2b["Pool IO"]
    CP2 --> NP2a
    CP2 --> NP2b
  end

  subgraph P3["P3 â€” Multi-clusters"]
    C1["Cluster A"]
    C2["Cluster B"]
  end

  L --> CP1
  CP1 --> CP2
  CP2 --> C1
  CP2 --> C2
```

> **Lecture** : on passe dâ€™un _monocluster mononÅ“ud_ Ã  des **pools de nÅ“uds** avec HA, puis Ã  des **multiâ€‘clusters** pour la rÃ©silience gÃ©ographique.

### 4) Vocabulaire minimal Â« prod Â»

- **Node pool** (tailles/machines dÃ©diÃ©es), **Cluster Autoscaler** (ajoute/retire des nÅ“uds).
- **HPA/VPA** (scaling des Pods), **PDB** (budgets de disruptions), **PodAntiAffinity** (rÃ©partition).
- **StorageClass/CSI**, **RWX/RWO**, **snapshot** & **backup**.
- **Ingress Controller** + **LoadBalancer**; **DNS** externe; **certâ€‘manager**.
- **RBAC**, **NetworkPolicy**, **PSA**, **Secret management** (KMS/External Secrets).

### 5) Checklist : passer de MicroK8s â†’ Prod

1. **Images** : base durcie, scans, registry privÃ©, tags immuables (digest).
2. **RÃ©seau** : choisir une **CNI** et dÃ©finir des **NetworkPolicy**.
3. **Stockage** : sÃ©lectionner un **CSI** adaptÃ© (performances / modes dâ€™accÃ¨s).
4. **SÃ©curitÃ©** : RBAC, PSA, secrets chiffrÃ©s, pull secrets, politiques dâ€™images.
5. **Exposition** : Ingress HA + certificat valide, DNS.
6. **ObservabilitÃ©** : mÃ©triques, logs, traces + alertes.
7. **DÃ©ploiements** : Helm + GitOps (environnement dev/stage/prod).
8. **RÃ©silience** : autoscaling nÅ“uds/Pods, PDB, backups, tests de reprise (DR).

> **Message clÃ© pour les TD** : MicroK8s donne la **mÃªme API** que la prod. Ce qui change, câ€™est **lâ€™Ã©chelle** et les **composants gÃ©rÃ©s** (rÃ©seau, stockage, sÃ©curitÃ©, HA).
