# CM3 â€“ Gestion de la durabilitÃ© et de lâ€™Ã©tat dans Kubernetes

## Bloc 1 enrichi â€” Du dÃ©ploiement Ã©phÃ©mÃ¨re Ã  la supervision du cluster

---

## 1. Contexte gÃ©nÃ©ral : du dÃ©ploiement Ã©phÃ©mÃ¨re Ã  la production

Dans Kubernetes, **les conteneurs sont par dÃ©finition Ã©phÃ©mÃ¨res** : ils peuvent Ãªtre dÃ©truits et recrÃ©Ã©s Ã  tout moment selon les besoins du cluster.
Cette caractÃ©ristique garantit la rÃ©silience, mais impose de comprendre **comment lâ€™Ã©tat et la durabilitÃ© sont gÃ©rÃ©s**.

> **Objectif du CM3** : comprendre comment Kubernetes maintient la stabilitÃ©, la santÃ© et la persistance des applications, malgrÃ© la nature Ã©phÃ©mÃ¨re des conteneurs.

Ce cours aborde :

- la **rÃ©action du cluster aux crashs** ;
- la **surveillance des applications** (readiness, liveness) ;
- la **gestion des ressources et de la persistance** ;
- et la **supervision du cluster lui-mÃªme**.

---

## 2. Gestion des crashs et de lâ€™Ã©tat des pods

### 2.1 Cycle de vie dâ€™un Pod

Un **Pod** encapsule un ou plusieurs conteneurs partageant le mÃªme rÃ©seau et stockage. Il traverse plusieurs Ã©tats :

| Ã‰tat                 | Description                                                           |
| -------------------- | --------------------------------------------------------------------- |
| **Pending**          | Le pod a Ã©tÃ© acceptÃ© mais ses conteneurs ne sont pas encore dÃ©marrÃ©s. |
| **Running**          | Tous les conteneurs sont lancÃ©s et fonctionnent.                      |
| **Succeeded**        | Tous les conteneurs se sont terminÃ©s sans erreur.                     |
| **Failed**           | Au moins un conteneur sâ€™est arrÃªtÃ© avec une erreur.                   |
| **CrashLoopBackOff** | Le conteneur Ã©choue et redÃ©marre en boucle.                           |

#### Commandes utiles

```bash
kubectl get pods -w
kubectl describe pod <nom>
kubectl logs <nom>
```

Exemple :

```bash
NAME           READY   STATUS             RESTARTS   AGE
mailpit-7d89   1/1     Running            0          5m
api-5f86d4c    0/1     CrashLoopBackOff   3          2m
```

### 2.2 Comportement en cas de crash

Lorsquâ€™un conteneur Ã©choue :

1. Le **kubelet** (agent de nÅ“ud) dÃ©tecte lâ€™Ã©chec.
2. Il consulte la **politique de redÃ©marrage** :

   ```yaml
   restartPolicy: Always
   ```

3. Il relance automatiquement le conteneur.

Ce mÃ©canisme assure une **auto-guÃ©rison basique** du cluster.

### 2.3 RÃ´le du runtime (Containerd)

Le kubelet sâ€™appuie sur le **container runtime** (souvent _Containerd_ ou _CRI-O_) pour :

- crÃ©er et exÃ©cuter les conteneurs ;
- gÃ©rer les systÃ¨mes de fichiers et rÃ©seaux ;
- superviser leur Ã©tat.

Sous Minikube :

```bash
minikube ssh
sudo ctr containers list
```

> Le conteneur peut subsister temporairement mÃªme aprÃ¨s la suppression du pod Kubernetes.

### 2.4 CrashLoopBackOff

Cet Ã©tat indique que Kubernetes tente pÃ©riodiquement de redÃ©marrer un conteneur en Ã©chec, avec une **dÃ©sactivation exponentielle** des tentatives.

```bash
Warning  BackOff  kubelet  Back-off restarting failed container
```

### 2.5 Nettoyage et persistance

- Les fichiers dans le conteneur (ex: `/tmp/data`) sont **perdus** Ã  la suppression.
- Seules les donnÃ©es montÃ©es sur un **volume persistant** (PersistentVolume) survivent.

> ğŸ’¡ _Nous verrons dans la suite comment rendre ces donnÃ©es persistantes._

---

## 2 bis. SantÃ© du cluster et supervision des pods systÃ¨mes

### 2.6 Namespace `kube-system`

Ce namespace contient les **pods essentiels** au fonctionnement du cluster.
Ils assurent les fonctions de rÃ©seau, de planification, de contrÃ´le et de stockage.

```bash
kubectl get pods -n kube-system
```

Exemples : CoreDNS, etcd, kube-apiserver, kube-scheduler, kubelet, etc.

### 2.7 Services vitaux du cluster

| Composant                      | RÃ´le principal                                                          |
| ------------------------------ | ----------------------------------------------------------------------- |
| **CoreDNS**                    | RÃ©solution interne de noms entre pods et services.                      |
| **etcd**                       | Base de donnÃ©es clÃ©-valeur stockant lâ€™Ã©tat du cluster.                  |
| **kube-apiserver**             | API du plan de contrÃ´le : interface et cohÃ©rence des donnÃ©es dans etcd. |
| **kube-scheduler**             | Affectation des pods aux nÅ“uds disponibles.                             |
| **controller-manager**         | Application des boucles de rÃ©conciliation.                              |
| **kube-proxy**                 | Routage du trafic entre services et pods.                               |
| **kindnet / calico / flannel** | Gestion du rÃ©seau inter-pods.                                           |
| **minikube addons manager**    | Gestion des extensions locales.                                         |
| **kubelet**                    | Agent exÃ©cutant les conteneurs sur chaque nÅ“ud.                         |

Observation :

```bash
kubectl logs <pod> -n kube-system
```

### 2.8 Configuration du plan de contrÃ´le

Les pods systÃ¨mes des nÅ“uds maÃ®tres sont dÃ©finis dans `/etc/kubernetes/manifests`.
Chaque fichier correspond Ã  un composant vital (API server, controller, scheduler...).

Exemple :

```bash
minikube ssh
ls /etc/kubernetes/manifests
cat kube-apiserver.yaml
```

- Si un fichier manifest est supprimÃ© â†’ le pod disparaÃ®t.
- Si le fichier est restaurÃ© â†’ le pod est recrÃ©Ã© automatiquement.

> âš™ï¸ **Le kubelet gÃ¨re ces pods statiques localement**, indÃ©pendamment de lâ€™API server.

### 2.9 Monitoring du cluster avec un DaemonSet (ex: Glances)

#### Origine du besoin

Kubernetes expose des mÃ©triques applicatives, mais il est souvent nÃ©cessaire de surveiller aussi les **ressources systÃ¨me** de chaque nÅ“ud.

#### Le DaemonSet

Un **DaemonSet** dÃ©ploie un pod sur chaque nÅ“ud du cluster.

Exemple : dÃ©ploiement de Glances pour superviser les performances locales.

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: glances
spec:
  selector:
    matchLabels:
      app: glances
  template:
    metadata:
      labels:
        app: glances
    spec:
      containers:
        - name: glances
          image: nicolargo/glances:latest
          ports:
            - containerPort: 61208
          volumeMounts:
            - mountPath: /var/run/docker.sock
              name: docker-sock
      volumes:
        - name: docker-sock
          hostPath:
            path: /var/run/docker.sock
```

#### ContrÃ´le du dÃ©ploiement

```bash
kubectl get daemonsets
kubectl get pods -l app=glances -o wide
```

#### TolÃ©rances et taints

Pour exÃ©cuter Glances sur les nÅ“uds maÃ®tres : ajouter des **tolerations**.

---

### SynthÃ¨se du bloc 1

- Les pods sont Ã©phÃ©mÃ¨res mais Kubernetes assure leur **auto-rÃ©cupÃ©ration**.
- Le **kubelet** et **Containerd** pilotent le cycle de vie des conteneurs.
- Le namespace `kube-system` contient les **services vitaux** du cluster.
- Les **manifests statiques** permettent de maintenir le plan de contrÃ´le.
- Les **DaemonSets** servent Ã  dÃ©ployer des agents de monitoring (ex : Glances) sur chaque nÅ“ud.

---

## Bloc 2 â€” Surveillance et gestion des ressources

---

## 3. Surveillance de lâ€™application : readiness et liveness probes

### 3.1 Pourquoi surveiller ?

Un pod peut Ãªtre **en cours dâ€™exÃ©cution** sans Ãªtre **fonctionnel**.
Les probes (ou sondes) permettent Ã  Kubernetes de **tester automatiquement la santÃ©** et la disponibilitÃ© des conteneurs.

Kubernetes dÃ©finit **trois types de sondes fonctionnelles**, chacune ayant un rÃ´le spÃ©cifique :

| Type de sonde       | Objectif principal                                       | Comportement                                                        |
| ------------------- | -------------------------------------------------------- | ------------------------------------------------------------------- |
| **Liveness Probe**  | VÃ©rifie si le conteneur est _toujours vivant_.           | RedÃ©marre le conteneur sâ€™il Ã©choue.                                 |
| **Readiness Probe** | VÃ©rifie si le conteneur est _prÃªt Ã  recevoir du trafic_. | Retire le pod du Service jusquâ€™Ã  rÃ©tablissement.                    |
| **Startup Probe**   | VÃ©rifie si le conteneur _a fini de dÃ©marrer_.            | Donne plus de temps au dÃ©marrage avant dâ€™activer les autres sondes. |

---

### ğŸ’¡ Ã€ propos des chemins `/healthz` et `/ready`

Dans Kubernetes (et dans de nombreux frameworks modernes), les applications exposent souvent des **points dâ€™entrÃ©e de supervision** :

| Endpoint                         | Usage courant | Description                                                       |
| -------------------------------- | ------------- | ----------------------------------------------------------------- |
| **`/healthz`**                   | Liveness      | VÃ©rifie que lâ€™application rÃ©pond encore â€” elle nâ€™est pas bloquÃ©e. |
| **`/ready`** ou **`/readiness`** | Readiness     | VÃ©rifie que lâ€™application est prÃªte Ã  recevoir des requÃªtes.      |
| **`/metrics`**                   | Monitoring    | Sert Ã  exporter des mÃ©triques vers Prometheus ou dâ€™autres outils. |

> Ces endpoints sont libres mais ont Ã©tÃ© **standardisÃ©s par convention** : Kubernetes nâ€™impose pas leur nom, mais beaucoup dâ€™outils et frameworks les reconnaissent.

---

### 3.2 MÃ©thodes techniques de probes

Chaque type de sonde (liveness, readiness, startup) peut utiliser lâ€™une des mÃ©thodes suivantes pour effectuer son test :

| MÃ©thode technique | Description                                            | Exemple              |
| ----------------- | ------------------------------------------------------ | -------------------- |
| **HTTP GET**      | VÃ©rifie la rÃ©ponse dâ€™un endpoint HTTP.                 | `/healthz`, `/ready` |
| **TCP Socket**    | VÃ©rifie quâ€™un port rÃ©seau est accessible.              | port 8080            |
| **Exec Command**  | ExÃ©cute une commande locale et vÃ©rifie le code retour. | `ps aux`             |

Exemple de dÃ©claration dans un manifest :

```yaml
livenessProbe:
  httpGet:
    path: /healthz
    port: 8080
  initialDelaySeconds: 10
  periodSeconds: 5

readinessProbe:
  tcpSocket:
    port: 8080
  initialDelaySeconds: 5
  periodSeconds: 5
```

---

### 3.3 Bonnes pratiques

- Toujours prÃ©voir un **dÃ©lai initial** (`initialDelaySeconds`) pour laisser lâ€™application dÃ©marrer.
- Adapter la frÃ©quence de test (`periodSeconds`) Ã  la durÃ©e moyenne de rÃ©ponse.
- DÃ©finir des seuils de tolÃ©rance (`failureThreshold`, `successThreshold`).
- Aligner les endpoints de test avec la logique applicative rÃ©elle.

> âš ï¸ Ã‰vitez les tests superflus : une sonde trop agressive peut redÃ©marrer inutilement un conteneur.

---

### 3.4 Exemple : application Flask

Une application Flask expose souvent un endpoint `/healthz` ou `/ping` pour les probes :

```python
from flask import Flask
app = Flask(__name__)

@app.route('/healthz')
def health():
    return 'OK', 200
```

La dÃ©claration de pod :

```yaml
containers:
  - name: web
    image: myapp:latest
    ports:
      - containerPort: 5000
    livenessProbe:
      httpGet:
        path: /healthz
        port: 5000
      initialDelaySeconds: 5
      periodSeconds: 5
```

RÃ©sultat : si Flask cesse de rÃ©pondre, Kubernetes redÃ©marre automatiquement le conteneur.

---

## 4. Gestion des ressources : CPU et mÃ©moire

### 4.1 Pourquoi limiter les ressources ?

Kubernetes mutualise les ressources entre plusieurs pods sur un mÃªme nÅ“ud.
DÃ©finir des **limites** et **demandes** permet :

- dâ€™assurer une **Ã©quitÃ© de rÃ©partition**,
- dâ€™Ã©viter quâ€™un pod monopolise le CPU ou la RAM,
- dâ€™amÃ©liorer la planification et la stabilitÃ© du cluster.

---

### 4.2 DÃ©claration des ressources

```yaml
resources:
  requests:
    cpu: "500m"
    memory: "256Mi"
  limits:
    cpu: "1"
    memory: "512Mi"
```

- `requests` : quantitÃ© garantie pour le conteneur.
- `limits` : plafond maximal autorisÃ©.

> ğŸ§® 1 CPU = 1000m (millicores). Une limite de `500m` correspond Ã  50 % dâ€™un cÅ“ur.

---

### 4.3 Effets des dÃ©passements

- Si la **mÃ©moire dÃ©passe** la limite â†’ le conteneur est tuÃ© (_OOMKill_).
- Si la **CPU dÃ©passe** la limite â†’ le conteneur est ralenti, mais pas tuÃ©.

Cas observables :

```bash
kubectl describe pod <nom>
# Chercher les Ã©vÃ©nements 'OOMKilled' ou 'Throttling CPU'
```

---

### 4.4 RÃ©servation et surallocation

Kubernetes permet une **surallocation contrÃ´lÃ©e** : plusieurs pods peuvent demander plus que la capacitÃ© totale du nÅ“ud, mais seuls les plus prioritaires seront servis selon la charge.

Câ€™est le rÃ´le du **scheduler**, qui arbitre les ressources disponibles.

---

### 4.5 PrioritÃ©s des pods

Kubernetes introduit des **PriorityClasses** pour dÃ©finir lâ€™ordre de traitement lors dâ€™une saturation :

```yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 1000
preemptionPolicy: PreemptLowerPriority
```

DÃ©claration dans un pod :

```yaml
spec:
  priorityClassName: high-priority
```

> ğŸ”¹ Si les ressources manquent, un pod de haute prioritÃ© peut **prÃ©empter** un autre de prioritÃ© infÃ©rieure.

---

### SynthÃ¨se du bloc 2

- Les **probes** garantissent la santÃ© et la disponibilitÃ© des pods.
- Les **liveness**, **readiness** et **startup** tests contrÃ´lent respectivement la vitalitÃ©, la disponibilitÃ© et le dÃ©marrage.
- Les **requests** et **limits** gÃ¨rent la consommation de ressources.
- Les **PriorityClasses** assurent la continuitÃ© des services essentiels.

> ğŸ’¡ Ces notions combinÃ©es permettent Ã  Kubernetes dâ€™assurer la **qualitÃ© de service (QoS)** et la **rÃ©silience applicative** du cluster.

---

## Bloc 3 â€” Persistance des donnÃ©es et StatefulSets (version finale consolidÃ©e)

---

## 5. Persistance des donnÃ©es

### 5.1 Pourquoi la persistance est-elle nÃ©cessaire ?

Les pods Kubernetes sont **Ã©phÃ©mÃ¨res** : lors dâ€™un redÃ©marrage, leur systÃ¨me de fichiers est reconstruit Ã  partir de lâ€™image du conteneur. Toute donnÃ©e stockÃ©e en local est donc **perdue**.

Pour les applications manipulant des donnÃ©es (bases de donnÃ©es, services de messagerie, journaux, etc.), il faut un moyen de **prÃ©server ces informations entre les cycles de vie des pods**.
Kubernetes propose pour cela un modÃ¨le de **volumes persistants** qui dÃ©couple la durÃ©e de vie du stockage de celle des pods.

---

## 5.2 CrÃ©ation manuelle : PV + PVC liÃ©s statiquement

### a. Ã‰tape 1 â€” DÃ©finir un PersistentVolume (PV)

Un **PersistentVolume** reprÃ©sente une ressource de stockage **physique ou logique** disponible dans le cluster.
Il peut correspondre Ã  un disque local, un partage NFS, un pÃ©riphÃ©rique iSCSI, etc.

Exemple :

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-demo
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: /data/demo
```

Ce volume est stockÃ© localement sur le nÅ“ud dans le rÃ©pertoire `/data/demo`.

---

### b. Ã‰tape 2 â€” CrÃ©er un PersistentVolumeClaim (PVC)

Un **PersistentVolumeClaim** est la **demande dâ€™un pod** pour un volume persistant.

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-demo
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
```

Lorsque le PVC est crÃ©Ã© :

- Kubernetes recherche un PV compatible (taille, mode dâ€™accÃ¨s, disponibilitÃ©) ;
- sâ€™il en trouve un, il le **lie automatiquement** (`Bound`).

VÃ©rification :

```bash
kubectl get pv,pvc
```

---

### c. Ã‰tape 3 â€” Utiliser le PVC dans un Pod

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: demo-pod
spec:
  containers:
    - name: demo
      image: busybox
      command: ["sleep", "3600"]
      volumeMounts:
        - mountPath: /mnt/data
          name: demo-volume
  volumes:
    - name: demo-volume
      persistentVolumeClaim:
        claimName: pvc-demo
```

Le pod aura accÃ¨s au rÃ©pertoire `/data/demo` du nÅ“ud sous `/mnt/data` dans le conteneur.

---

## 5.3 Du modÃ¨le statique au modÃ¨le dynamique (StorageClass)

La mÃ©thode prÃ©cÃ©dente exige de **crÃ©er manuellement chaque PV** avant de pouvoir le rÃ©clamer via un PVC.
Câ€™est lourd Ã  maintenir et inadaptÃ© aux environnements dynamiques (multi-nÅ“uds, cloud, etc.).

Pour pallier cela, Kubernetes introduit les **StorageClasses**, qui permettent le **provisionnement automatique** de volumes persistants.

### a. StorageClass : principe

Une **StorageClass** dÃ©finit _comment_ Kubernetes crÃ©e un volume :

- quel **provisioner** utiliser (ex. driver CSI, hostPath, EBS, Ceph, NFS, etc.) ;
- quelle **politique de rÃ©clamation** appliquer ;
- quel **moment** choisir pour lâ€™allocation (immÃ©diate ou diffÃ©rÃ©e).

Exemple :

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: standard
provisioner: k8s.io/minikube-hostpath
reclaimPolicy: Delete
volumeBindingMode: Immediate
```

---

### b. PVC dynamique

Lorsquâ€™un PVC fait rÃ©fÃ©rence Ã  une StorageClass, Kubernetes **crÃ©e automatiquement un PV** adaptÃ© Ã  la demande :

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-dyn
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi
  storageClassName: standard
```

Ã€ la crÃ©ation du PVC :

- un nouveau PV est gÃ©nÃ©rÃ© automatiquement par le provisioner spÃ©cifiÃ© ;
- le PVC et le PV sont liÃ©s (`Bound`).

Aucune dÃ©finition de PV nâ€™est requise manuellement.

VÃ©rification :

```bash
kubectl get pv,pvc
```

> ğŸ’¡ Lâ€™administrateur ne dÃ©clare plus de volumes Ã  lâ€™avance : Kubernetes provisionne Ã  la volÃ©e via la StorageClass.

---

### âš ï¸ Limite importante du provisionnement dynamique

> Le provisionnement dynamique **ne garantit pas la portabilitÃ© du stockage** entre les nÅ“uds.
>
> - Si la StorageClass repose sur un **stockage en rÃ©seau** (NFS, CephFS, Longhorn, OpenEBS, CSI Cloud, etc.), le volume est accessible depuis nâ€™importe quel nÅ“ud.
> - Si elle repose sur un **disque local** (`hostPath`, `local`), le PV reste attachÃ© Ã  un seul nÅ“ud. Kubernetes doit planifier le pod sur ce mÃªme nÅ“ud pour quâ€™il puisse accÃ©der Ã  ses donnÃ©es.
>   Ce comportement est gÃ©rÃ© via `WaitForFirstConsumer` et `nodeAffinity`.
>
> ğŸ‘‰ En rÃ©sumÃ© : le modÃ¨le dynamique est efficace lorsquâ€™un **backend rÃ©seau** rend les volumes accessibles Ã  tous les nÅ“uds du cluster.

---

### c. Comparatif entre volumes statiques et dynamiques

| Aspect             | PV/PVC statiques                  | StorageClass dynamique           |
| ------------------ | --------------------------------- | -------------------------------- |
| CrÃ©ation du volume | Manuelle (PV dÃ©fini avant le PVC) | Automatique par Kubernetes       |
| Liens PVâ€“PVC       | Correspondance obligatoire        | GÃ©rÃ©s par le provisioner         |
| PortabilitÃ©        | LimitÃ©e (souvent locale)          | Optimale avec stockage rÃ©seau    |
| Cas dâ€™usage        | DÃ©monstration, disque local       | Clusters multi-nÅ“uds, cloud, CSI |

---

## 5.4 Modes dâ€™accÃ¨s, sÃ©curitÃ© et typologie des stockages

### a. Modes dâ€™accÃ¨s disponibles

| Mode                        | Signification                         | Cas dâ€™usage                    |
| --------------------------- | ------------------------------------- | ------------------------------ |
| **ReadWriteOnce (RWO)**     | Lecture/Ã©criture par un seul nÅ“ud.    | Disque local, base de donnÃ©es. |
| **ReadOnlyMany (ROX)**      | Lecture seule depuis plusieurs nÅ“uds. | Fichiers statiques, images.    |
| **ReadWriteMany (RWX)**     | Lecture/Ã©criture par plusieurs nÅ“uds. | NFS, CephFS, GlusterFS.        |
| **ReadWriteOncePod (RWOP)** | AccÃ¨s exclusif par un seul pod.       | SÃ©curitÃ© renforcÃ©e.            |

### b. SÃ©curitÃ© et permissions

Lâ€™accÃ¨s en Ã©criture dÃ©pend des droits POSIX sur le volume montÃ©.
Il est possible de spÃ©cifier un **securityContext** :

```yaml
securityContext:
  runAsUser: 1000
  fsGroup: 1000
```

Ainsi, le systÃ¨me de fichiers applique les permissions correspondant Ã  lâ€™utilisateur du conteneur.

### c. Types de backend

| Type                 | Description                          | ParticularitÃ©s                                          |
| -------------------- | ------------------------------------ | ------------------------------------------------------- |
| **hostPath / local** | Stockage local du nÅ“ud.              | Rapide, mais non partagÃ©.                               |
| **NFS**              | Partage rÃ©seau simple.               | Compatible RWX, facile Ã  configurer.                    |
| **CSI Driver**       | Interface standard pour le stockage. | Supporte de nombreux backends (Ceph, EBS, Azure, etc.). |
| **CephFS / RBD**     | Stockage distribuÃ© en rÃ©seau.        | Haute disponibilitÃ©, compatible VM.                     |
| **Cloud storage**    | EBS (AWS), Persistent Disk (GCP)...  | Provisionnement dynamique complet.                      |

---

## 6. StatefulSets et bases de donnÃ©es

### 6.1 Limites des Deployments

Les **Deployments** conviennent aux applications _stateless_.
Pour les bases de donnÃ©es, caches et services nÃ©cessitant une identitÃ© stable, on utilise un **StatefulSet**.

> âš ï¸ Kubernetes ne gÃ¨re pas la cohÃ©rence applicative : la rÃ©plication, la concurrence en Ã©criture et la synchronisation sont du ressort du moteur de base de donnÃ©es.

---

### 6.2 CaractÃ©ristiques dâ€™un StatefulSet

| Fonction        | Description                                                    |
| --------------- | -------------------------------------------------------------- |
| IdentitÃ© stable | Les pods sont nommÃ©s sÃ©quentiellement (`app-0`, `app-1`, ...). |
| Volume dÃ©diÃ©    | Chaque pod possÃ¨de son propre PVC.                             |
| Ordre contrÃ´lÃ©  | CrÃ©ation et suppression ordonnÃ©es.                             |
| StabilitÃ©       | Chaque pod conserve son nom et son volume aprÃ¨s redÃ©marrage.   |

---

### 6.3 Exemple : MariaDB avec StatefulSet

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mariadb
spec:
  selector:
    matchLabels:
      app: mariadb
  serviceName: mariadb
  replicas: 1
  template:
    metadata:
      labels:
        app: mariadb
    spec:
      containers:
        - name: mariadb
          image: mariadb:10.11
          env:
            - name: MYSQL_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: mariadb-secret
                  key: root-password
          ports:
            - containerPort: 3306
          volumeMounts:
            - name: data
              mountPath: /var/lib/mysql
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes: ["ReadWriteOnce"]
        resources:
          requests:
            storage: 2Gi
```

#### Points clÃ©s :

- `volumeClaimTemplates` crÃ©e un **PVC par pod** (`data-mariadb-0`, `data-mariadb-1`, ...).
- Si `replicas > 1`, chaque instance est indÃ©pendante sauf configuration de rÃ©plication.
- Pour la cohÃ©rence :
  - utiliser une rÃ©plication interne (ex : **MariaDB Galera**, **MySQL Group Replication**) ;
  - exposer les pods via un **Service Headless** (`clusterIP: None`) ;
  - gÃ©rer les transactions cÃ´tÃ© SGBD.

---

### 6.4 VÃ©rification et gestion

```bash
kubectl get statefulsets
kubectl get pods -l app=mariadb
kubectl get pvc | grep mariadb
```

Chaque pod possÃ¨de son volume personnel.
Le redimensionnement :

```bash
kubectl scale statefulset mariadb --replicas=3
```

> Kubernetes orchestre les pods et volumes, **pas la logique transactionnelle**.

---

### 6.5 ConfigMaps et Secrets

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: mariadb-secret
type: Opaque
data:
  root-password: bXlzZWNyZXRwYXNzCg== # base64("mysecretpass")
```

Les **Secrets** et **ConfigMaps** permettent de stocker la configuration et les mots de passe de maniÃ¨re sÃ©curisÃ©e.
Ils sont _namespaced_ et non persistants, mais liÃ©s au cycle de vie du dÃ©ploiement.

---

### ğŸ§  SynthÃ¨se du bloc 3

- Les **PV/PVC** assurent la persistance des donnÃ©es.
- Les **StorageClasses** permettent un **provisionnement dynamique** des volumes.
- Cette automatisation nâ€™est pertinente que si le **stockage est en rÃ©seau** et accessible Ã  tous les nÅ“uds.
- Les **modes dâ€™accÃ¨s**, **droits**, et **types de backend** dÃ©terminent la souplesse du stockage.
- Les **StatefulSets** gÃ¨rent la stabilitÃ© des applications avec Ã©tat, mais la **cohÃ©rence des donnÃ©es** relÃ¨ve des moteurs applicatifs.

> ğŸ’¡ Kubernetes ne se limite pas Ã  redÃ©marrer des conteneurs : il orchestre la **durabilitÃ©**, la **stabilitÃ©** et la **persistance** des applications au sein dâ€™environnements distribuÃ©s.

---
